<!DOCTYPE html>
<html lang="en">
<head>
      <title>▲HEAP</title>
    <meta charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="http://marshallmallicoat.com/theme/css/style.css" />
 <!-- adds article description to head, if available -->
</head>

<body>
  <header>
      <h1><a href="http://marshallmallicoat.com/" title="Home">▲HEAP</a></h1>
  </header>

  <main>
<article>
  <header><h1>Predicting House Prices</h1></header>
  <footer>
        Published
        <time class="published" datetime="2018-11-12T00:00:00-06:00">
        12 Nov 2018</time>
  </footer>

  <div class="section" id="introduction">
<h2>Introduction</h2>
<p>The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on <a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">predicting house prices</a>.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms, the size of the yard, and so on.
The data are from houses in Ames, Iowa, compiled for use in data
science education.</p>
<p>To solve the problem, I developed a Generalized Linear Regression
(GLM) model. The GLM model works by fitting a function to the
features of houses with known prices; then, to predict the price
of an additional house, the estimated function is evaluated with
the house's particular features. The function takes the form of a
<em>hyperplane</em> (a generalization of the plane to higher dimensional
space). Usually the response variable, in our case the house
price, is transformed before the fitting takes place.</p>
<p>The code for this project can be found on <a class="reference external" href="https://github.com/mmallicoat/kaggle-house-prices">Github</a>.</p>
</div>
<div class="section" id="data-prep">
<h2>Data Prep</h2>
<p>The first step I took was to stratify the data provided,
complete with the price of each house, into training and
cross-validation (CV) datasets. Having a CV dataset lets you
compare the performance of models on out-of-sample data, thereby
avoiding overfitting and getting a more accurate estimate of its
performance. I randomly partitioned the labeled dataset into the
training and CV datasets, since I do not know if there is some
order to how they are presented in the file Kaggle provides which
might bias my model.</p>
<p>Next, I dealt with any missing values in the dataset. For numeric
variables, I calculated the mean of the variable in the training
data and substituted this value for any missing in the training
and CV datasets. For the categorical variables, I substituted a
new value &quot;Unknown.&quot;</p>
<p>To select from the many features, I used some simple heuristics.
For the numeric variables, I calculated the variance of each
within the training data. I produced a scatter plot of the
variances of the variables, sorted in ascending order. In this
plot, I found an &quot;elbow&quot; where there was a significant drop-off
in variance. I then selected all variables with variance above
this threshold. This amounted to 12 variables, about a third of
the numeric variables available. The rationale behind this is that
variables with low variance do not provide much discriminating
information between houses, since all of the houses will have
similar values.</p>
<div class="figure align-center">
<img alt="plot of variances of numeric variables" src="./figures/numeric-selection.png" />
<p class="caption">&quot;Elbow&quot; in the plot of variances of numeric variables</p>
</div>
<p>For each of the categorical variables, I calculated the <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a>,
assuming each value was pulled from a multinomial distribution.
Entropy is a measure of the amount of &quot;information&quot; contained in
a stochastic process. Random variables with little &quot;surprise&quot; in
their realized values will have low entropy. For binary variables,
the entropy calculated is equivalent to the variance of the
corresponding Bernoulli distribution. After calculating the
entropy, I plotted a histogram, found an &quot;elbow&quot; is use for the
threshold, and selected all of the variables above this threshold,
in the same manner as the numeric variables. This amounted to 14
variables, about a third of the categorical variables available.</p>
<div class="figure align-center">
<img alt="plot of entropy of categorical variables" src="./figures/categorical-selection.png" />
<p class="caption">&quot;Elbow&quot; in the histogram of entropy of categorical variables</p>
</div>
<p>The selected categorical variables were then encoded as dummy
variables, <a class="footnote-reference" href="#id2" id="id1">[1]</a> so that they can be included in the regression.
One trip-up was that there are values of categorical variables
appearing in the test data that do not appear in the training/CV
data. The universe of values must be known ahead of time in order
to encode the variable as dummy variables. Given a new dataset
containing unseen values of a categorical variable, the model
could not be applied.</p>
<p>It is somewhat challenging to develop a processing pipeline
that can be applied to all datasets uniformly, particularly
when the processing procedure is &quot;fitted&quot; to the training data.
The processing must then be applied to training data first, the
parameters estimated, and those parameters stored somewhere so
that they can be applied when processing the CV and testing
datasets. Instead of writing out the parameters to disk, as I did
for the scaler and regression parameters, I simply kept them in
memory and processed all of the datasets at once. This is less
than ideal, since I wouldn't have the parameters readily available
to apply to a new dataset, which would be necessary if this model
were used in an application.</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Dummy variables are a collection of binary variables whose
combination correspond to one of the values of the categorical
variable. The simplest example is a variable with possible values
&quot;Male&quot; and &quot;Female&quot; being encoded as 1 and 0. For variables with
<em>n</em> possible values, <em>n - 1</em> dummy variables are required.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="training">
<h2>Training</h2>
<p>Before training the model, I performed some transformations on
the data. I standardized the features, subtracting the mean and
dividing by standard deviation to create features with zero mean
and unit variance. If the features have different scales, the
magnitude of the fitted coefficients in the linear model will be
influenced by the scale of the underlying variables and harder to
compare. Coefficients of variables with a larger scale would also
be penalized more highly if regularization is applied.</p>
<p>Another transformation was to take the logarithm of the sale
price response variable. There are two reasons for this: First,
like most currency values, the house prices in the data are not
normally distributed, which violates an assumption of the linear
regression. This can be seen in the histogram below over which
I've overlaid a fitted normal distribution.</p>
<div class="figure align-center">
<img alt="histogram of house prices" src="./figures/y-hist.png" />
</div>
<p>By log-transforming the response variable, it is much closer to
following a normal distribution. <a class="footnote-reference" href="#id4" id="id3">[2]</a></p>
<div class="figure align-center">
<img alt="histogram of log-transformed house prices" src="./figures/y-transformed-hist.png" />
</div>
<p>Secondly, the loss function specified for the Kaggle competition
is the mean squared error of the <em>logarithm</em> of the house prices
predicted. If we wish to develop a model that performs well under
this loss function, we must optimize the parameters of our model
with respect to it.</p>
<p>My initial model resulted in some very large positive and negative
coefficients in the fitted model. Due to the limited precision
of floating point arithmetic, these coefficients lead to some
overflows and <a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow">underflows</a>, respectively, in the predicted value of
the response variable. To remedy this, I used a ridge regression
instead, which adds a regularization term to the loss function
used for fitting the model, thereby penalizing coefficients with
large magnitude. This solved the problem of unreasonable large
coefficients.</p>
<p>After training the model, I saved the parameters of both the
standardization procedure and the linear regression. These are
both are needed in order to repeat the preprocessing steps and
make predictions from the CV and test datasets.</p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[2]</a></td><td>Processes that are the sum of many independent occurrences
generally follow a normal distribution, which is consistence with
the Central Limit Theorem. An example of this is human height,
which is perhaps the result of the expression of many different
genes, the quality of nutrition through each phase of childhood,
the effects of childhood disease, etc., which are generally
independence events, each having a small effect. Processes like
prices or salaries cannot be normally distributed on the face
since they cannot have negative values. Secondly, instead of the
constituents having an additive effect, they seem to have more of
a <em>multiplicative</em> effect on the outcome. Learning two new skills
will increase your salary more than that sum of each alone.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="prediction">
<h2>Prediction</h2>
<p>To make predictions given the CV and test datasets, the
preprocessing steps and repeated:</p>
<ol class="arabic simple">
<li>Standardize the variables using means and standard deviations
from training dataset</li>
<li>For the CV dataset, log-transform the response variable. (We do
not know the value of the response variable for the testing data,
of course.)</li>
<li>Apply our regression model to make a prediction: multiply
values of the features by the fitted coefficients, sum these up,
and add the intercept.</li>
<li>For the CV dataset, calculate the value of the loss function as
a diagnostic.</li>
<li>Before writing out the predictions, reverse the log-transform
by exponentiating the predicted value.</li>
</ol>
<p>The Kaggle competition is judged by the square root of the mean
squared error (RMSE) of the predictions of the log-transformed
house prices. This metric for our model (on the test dataset) is
0.168, which is fairly middling compared to the leaderboard on the
Kaggle website. For the CV dataset, the metric is 0.166, which is
close to that of the test dataset, as we would expect.</p>
<p>The metric is somewhat difficult to interpret, so I calculated the
RMSE of the <em>un</em>-transformed prices for comparison. The RMSE for
the untransformed prices in the CV dataset is $37,576. This is
very roughly <a class="footnote-reference" href="#id6" id="id5">[3]</a> the expected deviation of our prediction from
the true price. The mean house price in this dataset is $178,186;
so, although our error is significant, the predictions are within
the ballpark of the true values.</p>
<p>There are many avenues to explore which could improve the
model's performance. Here are some things to try in the future:</p>
<ul class="simple">
<li>Engineer some custom features, especially ones that capture
interactions between variables. These might be something like the
ratio of bathrooms to bedrooms, or ratio of plot area to house
floor area.</li>
<li>Make use of the ordinal variables: there are some variables that
are actually ordinal, not categorical. An example of this is X.
Instead of ignoring the ordering of the levels of the variable,
they could be taken advantage of.</li>
<li>Try some alternate models, especially those that can fit
non-linear functions. There may be some non-linear interactions
between the house price and the independent variables, such as
the price not being monotonically increasing with the value of an
independence variable. One plausible explanation of this might be
something along the lines of: a larger yard may correlate with a
more valuable property, but it may correlate with a more rural
location; the negative effect of the rural location on the house
price might outweigh the increase from the larger yard.</li>
<li>Supplement external data: we are given the names of
neighborhoods of the houses. There is publicly available data on
houses and their prices from these locations. This data could be
collected and used to supplement the data provided by Kaggle. Or,
a secondary model could be built from the external data and then
combined with the model trained on the Kaggle data in an ensemble.</li>
</ul>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[3]</a></td><td>The RMSE is in fact the standard deviation of the
residuals, which are the differences between each prediction and
true value. The standard deviation is the square root of the
expected squared deviation, rather than the expected deviation.</td></tr>
</tbody>
</table>
</div>


</article>
  </main>

  <hr>
  <footer>
  <nav>
    <h3>Navigation</h3>
    <ul>
      <li><a href="http://marshallmallicoat.com/">Home</a></li>
      <li><a href="http://marshallmallicoat.com/about.html">About</a></li>
      <li><a href="http://marshallmallicoat.com/feed.xml">Feed</a></li>
    </ul>
  </nav>
  </footer>
</body>
</html>