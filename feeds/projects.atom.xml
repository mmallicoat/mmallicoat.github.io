<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>▲HEAP - Projects</title><link href="http://marshallmallicoat.com/" rel="alternate"></link><link href="http://marshallmallicoat.com/feeds/projects.atom.xml" rel="self"></link><id>http://marshallmallicoat.com/</id><updated>2018-11-12T00:00:00-06:00</updated><entry><title>Predicting House Prices</title><link href="http://marshallmallicoat.com/kaggle-house-prices.html" rel="alternate"></link><published>2018-11-12T00:00:00-06:00</published><updated>2018-11-12T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-12:/kaggle-house-prices.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms, the size of the yard, and so on.
The data are from houses in Ames, Iowa, compiled for use in data
science education.&lt;/p&gt;
&lt;p&gt;To solve the problem, I developed a Generalized Linear Regression
(GLM) model. The GLM model works by fitting a function to the
features of houses with known prices; then, to predict the price
of an additional house, the estimated function is evaluated with
the house's particular features. The function takes the form of a
&lt;em&gt;hyperplane&lt;/em&gt; (a generalization of the plane to higher dimensional
space). Usually the response variable, in our case the house
price, is transformed before the fitting takes place.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/kaggle-house-prices"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="data-prep"&gt;
&lt;h2&gt;Data Prep&lt;/h2&gt;
&lt;p&gt;The first step I took was to stratify the data provided,
complete with the price of each house, into training and
cross-validation (CV) datasets. Having a CV dataset lets you
compare the performance of models on out-of-sample data, thereby
avoiding overfitting and getting a more accurate estimate of its
performance. I randomly partitioned the labeled dataset into the
training and CV datasets, since I do not know if there is some
order to how they are presented in the file Kaggle provides which
might bias my model.&lt;/p&gt;
&lt;p&gt;Next, I dealt with any missing values in the dataset. For numeric
variables, I calculated the mean of the variable in the training
data and substituted this value for any missing in the training
and CV datasets. For the categorical variables, I substituted a
new value &amp;quot;Unknown.&amp;quot;&lt;/p&gt;
&lt;p&gt;To select from the many features, I used some simple heuristics.
For the numeric variables, I calculated the variance of each
within the training data. I produced a scatter plot of the
variances of the variables, sorted in ascending order. In this
plot, I found an &amp;quot;elbow&amp;quot; where there was a significant drop-off
in variance. I then selected all variables with variance above
this threshold. This amounted to 12 variables, about a third of
the numeric variables available. The rationale behind this is that
variables with low variance do not provide much discriminating
information between houses, since all of the houses will have
similar values.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of variances of numeric variables" src="./figures/numeric-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the plot of variances of numeric variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For each of the categorical variables, I calculated the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;entropy&lt;/a&gt;,
assuming each value was pulled from a multinomial distribution.
Entropy is a measure of the amount of &amp;quot;information&amp;quot; contained in
a stochastic process. Random variables with little &amp;quot;surprise&amp;quot; in
their realized values will have low entropy. For binary variables,
the entropy calculated is equivalent to the variance of the
corresponding Bernoulli distribution. After calculating the
entropy, I plotted a histogram, found an &amp;quot;elbow&amp;quot; is use for the
threshold, and selected all of the variables above this threshold,
in the same manner as the numeric variables. This amounted to 14
variables, about a third of the categorical variables available.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of entropy of categorical variables" src="./figures/categorical-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the histogram of entropy of categorical variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The selected categorical variables were then encoded as dummy
variables, &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt; so that they can be included in the regression.
One trip-up was that there are values of categorical variables
appearing in the test data that do not appear in the training/CV
data. The universe of values must be known ahead of time in order
to encode the variable as dummy variables. Given a new dataset
containing unseen values of a categorical variable, the model
could not be applied.&lt;/p&gt;
&lt;p&gt;It is somewhat challenging to develop a processing pipeline
that can be applied to all datasets uniformly, particularly
when the processing procedure is &amp;quot;fitted&amp;quot; to the training data.
The processing must then be applied to training data first, the
parameters estimated, and those parameters stored somewhere so
that they can be applied when processing the CV and testing
datasets. Instead of writing out the parameters to disk, as I did
for the scaler and regression parameters, I simply kept them in
memory and processed all of the datasets at once. This is less
than ideal, since I wouldn't have the parameters readily available
to apply to a new dataset, which would be necessary if this model
were used in an application.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Dummy variables are a collection of binary variables whose
combination correspond to one of the values of the categorical
variable. The simplest example is a variable with possible values
&amp;quot;Male&amp;quot; and &amp;quot;Female&amp;quot; being encoded as 1 and 0. For variables with
&lt;em&gt;n&lt;/em&gt; possible values, &lt;em&gt;n - 1&lt;/em&gt; dummy variables are required.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="training"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;Before training the model, I performed some transformations on
the data. I standardized the features, subtracting the mean and
dividing by standard deviation to create features with zero mean
and unit variance. If the features have different scales, the
magnitude of the fitted coefficients in the linear model will be
influenced by the scale of the underlying variables and harder to
compare. Coefficients of variables with a larger scale would also
be penalized more highly if regularization is applied.&lt;/p&gt;
&lt;p&gt;Another transformation was to take the logarithm of the sale
price response variable. There are two reasons for this: First,
like most currency values, the house prices in the data are not
normally distributed, which violates an assumption of the linear
regression. This can be seen in the histogram below over which
I've overlaid a fitted normal distribution.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of house prices" src="./figures/y-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;By log-transforming the response variable, it is much closer to
following a normal distribution. &lt;a class="footnote-reference" href="#id4" id="id3"&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of log-transformed house prices" src="./figures/y-transformed-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Secondly, the loss function specified for the Kaggle competition
is the mean squared error of the &lt;em&gt;logarithm&lt;/em&gt; of the house prices
predicted. If we wish to develop a model that performs well under
this loss function, we must optimize the parameters of our model
with respect to it.&lt;/p&gt;
&lt;p&gt;My initial model resulted in some very large positive and negative
coefficients in the fitted model. Due to the limited precision
of floating point arithmetic, these coefficients lead to some
overflows and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow"&gt;underflows&lt;/a&gt;, respectively, in the predicted value of
the response variable. To remedy this, I used a ridge regression
instead, which adds a regularization term to the loss function
used for fitting the model, thereby penalizing coefficients with
large magnitude. This solved the problem of unreasonable large
coefficients.&lt;/p&gt;
&lt;p&gt;After training the model, I saved the parameters of both the
standardization procedure and the linear regression. These are
both are needed in order to repeat the preprocessing steps and
make predictions from the CV and test datasets.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Processes that are the sum of many independent occurrences
generally follow a normal distribution, which is consistence with
the Central Limit Theorem. An example of this is human height,
which is perhaps the result of the expression of many different
genes, the quality of nutrition through each phase of childhood,
the effects of childhood disease, etc., which are generally
independence events, each having a small effect. Processes like
prices or salaries cannot be normally distributed on the face
since they cannot have negative values. Secondly, instead of the
constituents having an additive effect, they seem to have more of
a &lt;em&gt;multiplicative&lt;/em&gt; effect on the outcome. Learning two new skills
will increase your salary more than that sum of each alone.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="prediction"&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;p&gt;To make predictions given the CV and test datasets, the
preprocessing steps and repeated:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Standardize the variables using means and standard deviations
from training dataset&lt;/li&gt;
&lt;li&gt;For the CV dataset, log-transform the response variable. (We do
not know the value of the response variable for the testing data,
of course.)&lt;/li&gt;
&lt;li&gt;Apply our regression model to make a prediction: multiply
values of the features by the fitted coefficients, sum these up,
and add the intercept.&lt;/li&gt;
&lt;li&gt;For the CV dataset, calculate the value of the loss function as
a diagnostic.&lt;/li&gt;
&lt;li&gt;Before writing out the predictions, reverse the log-transform
by exponentiating the predicted value.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Kaggle competition is judged by the square root of the mean
squared error (RMSE) of the predictions of the log-transformed
house prices. This metric for our model (on the test dataset) is
0.168, which is fairly middling compared to the leaderboard on the
Kaggle website. For the CV dataset, the metric is 0.166, which is
close to that of the test dataset, as we would expect.&lt;/p&gt;
&lt;p&gt;The metric is somewhat difficult to interpret, so I calculated the
RMSE of the &lt;em&gt;un&lt;/em&gt;-transformed prices for comparison. The RMSE for
the untransformed prices in the CV dataset is $37,576. This is
very roughly &lt;a class="footnote-reference" href="#id6" id="id5"&gt;[3]&lt;/a&gt; the expected deviation of our prediction from
the true price. The mean house price in this dataset is $178,186;
so, although our error is significant, the predictions are within
the ballpark of the true values.&lt;/p&gt;
&lt;p&gt;There are many avenues to explore which could improve the
model's performance. Here are some things to try in the future:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Engineer some custom features, especially ones that capture
interactions between variables. These might be something like the
ratio of bathrooms to bedrooms, or ratio of plot area to house
floor area.&lt;/li&gt;
&lt;li&gt;Make use of the ordinal variables: there are some variables that
are actually ordinal, not categorical. An example of this is X.
Instead of ignoring the ordering of the levels of the variable,
they could be taken advantage of.&lt;/li&gt;
&lt;li&gt;Try some alternate models, especially those that can fit
non-linear functions. There may be some non-linear interactions
between the house price and the independent variables, such as
the price not being monotonically increasing with the value of an
independence variable. One plausible explanation of this might be
something along the lines of: a larger yard may correlate with a
more valuable property, but it may correlate with a more rural
location; the negative effect of the rural location on the house
price might outweigh the increase from the larger yard.&lt;/li&gt;
&lt;li&gt;Supplement external data: we are given the names of
neighborhoods of the houses. There is publicly available data on
houses and their prices from these locations. This data could be
collected and used to supplement the data provided by Kaggle. Or,
a secondary model could be built from the external data and then
combined with the model trained on the Kaggle data in an ensemble.&lt;/li&gt;
&lt;/ul&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The RMSE is in fact the standard deviation of the
residuals, which are the differences between each prediction and
true value. The standard deviation is the square root of the
expected squared deviation, rather than the expected deviation.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content></entry><entry><title>Analyzing Subreddit Networks</title><link href="http://marshallmallicoat.com/subreddit-networks.html" rel="alternate"></link><published>2018-11-02T00:00:00-05:00</published><updated>2018-11-12T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-02:/subreddit-networks.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form a
network of closely related online communities. Because these
lists of links are human-curated, they give high-quality
indicators of related topics and communities. By following these
links programmatically, we can collect the data needed to to
visualize and analyze these networks.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/subreddit-graphs"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="scrape-the-data"&gt;
&lt;h2&gt;Scrape the Data&lt;/h2&gt;
&lt;p&gt;I wrote simple web scraper using the Python library &lt;tt class="docutils literal"&gt;scrapy&lt;/tt&gt;
which collects the list of links and other information from each
subreddit page. Starting from an initial subreddit, the scraper
searches the sidebar on the page for any related subreddits that
have been linked there. The scraper then follows those links and
iteratively searches for more links. The name of each subreddit is
collected, along with its description, number of subscribers, and
the list of links to other subreddits.&lt;/p&gt;
&lt;p&gt;From these data, it is possible to construct the network of
subreddits surrounding the initial page. In this network (often
called a &lt;em&gt;graph&lt;/em&gt; in mathematics), each node is a subreddit and
each edge is a hyperlink between them. For simplicity, I decided
to build a non-directed graph, ignoring the directionality of
hyperlinks: a hyperlink from subreddit A to B is treated in the
same manner as a hyperlink from subreddit B to A.&lt;/p&gt;
&lt;p&gt;The scraper uses CSS selectors to locate the desired elements on
each webpage. Here is a code snippet with the selectors:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sub_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subscriber_conversion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-12::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# convert from string to numeric&lt;/span&gt;
    &lt;span class="n"&gt;links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0 a::attr(href)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="s1"&gt;'subreddit'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'description'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-14::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'links'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="s1"&gt;'subscribers'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sub_count&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The sidebar on the page is identified using the method
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;response.css('div.s1s8pi67-0')&lt;/span&gt;&lt;/tt&gt; where &lt;tt class="docutils literal"&gt;response&lt;/tt&gt; is the
object representing the returned webpage. The string
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;s1s8pi67-0&lt;/span&gt;&lt;/tt&gt; is a unique class of the &lt;tt class="docutils literal"&gt;div&lt;/tt&gt; element containing
the sidebar. &lt;a class="footnote-reference" href="#selectors" id="id1"&gt;[1]&lt;/a&gt; The scaper returns the name of the
subreddit, the subreddit's description, the list of links in the
sidebar, and the number of subscribers.&lt;/p&gt;
&lt;p&gt;I chose two subreddits to use as the starting points to crawl
through related pages, which results in two networks for analysis:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;one centered at /r/programming, which I will refer
to as the &amp;quot;programming network&amp;quot;&lt;/li&gt;
&lt;li&gt;one centered at /r/financialindependence, which I
will refer to as the &amp;quot;financial network.&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these initial subreddits have a fair number of links
listed in their sidebars, which should lead to larger, more
complex networks.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="selectors" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Using these seemingly random attribute values for
the selectors is less than ideal: they are non-semantic and they
seem to change fairly frequently, possibly with each build of the
website. An improvement woudl be to find a more robust selector. I
think it would be possible to use an XPath selector to find the
text &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/r/[subreddit&lt;/span&gt; name]&lt;/tt&gt; that appears in the sidebar, and then
select the document element containing this a few steps up the
hierarchy.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="visualizations"&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;p&gt;After collecting the network data, we can use the library
&lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt; to visualize and analyze the networks. I quickly
made a couple plots using &lt;tt class="docutils literal"&gt;matplotlib&lt;/tt&gt; to visualize the graphs.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="network centered as /r/programming" src="./figures/prog-graph.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Unfortunately, these were difficult to read and not very useful
for exploring the networks. To remedy this, I decided to use the
visualization library &lt;tt class="docutils literal"&gt;d3&lt;/tt&gt; (written in JavaScript) to make some
interactive plots. After we convert the network data into the
&amp;quot;node-link&amp;quot; JSON format using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can read it into a
HTML file containing JavaScript visualizations.&lt;/p&gt;
&lt;p&gt;Click on the images below to view the interactive plots.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/fin-force.html"&gt;&lt;img alt="network centered as /r/financialindependence" src="./figures/fin-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/financialindependence&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/prog-force.html"&gt;&lt;img alt="network centered as /r/programming" src="./figures/prog-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/programming&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In these charts, the relative number of subscribers to each
subreddit is represented by the radius of the node (using a
log-scale).&lt;/p&gt;
&lt;p&gt;One of the most salient features is the &amp;quot;spoke-and-hub&amp;quot; structure:
a larger subreddit links to many smaller subreddits, which
are often dedicated to a more specific topic. For example,
/r/financialindependence is linked to country-specific subreddits
for Canada (/r/personalfinancecanada), UK (/r/ukpersonalfinance),
and so on. These can often be sensibly grouped into a cluster of
nodes based on their subject matter. An example of this is the
closely related subreddits surrounding /r/collapse which are all
dedicated to the topic of societal and economic collapse.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis"&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;div class="section" id="average-degree-and-density"&gt;
&lt;h3&gt;Average Degree and Density&lt;/h3&gt;
&lt;p&gt;One descriptive statistic of a graph is the &lt;em&gt;average degree&lt;/em&gt;. The
degree of a node is the number of edges connected to it. The
average degree of a graph is simply the average of the degrees of
each of its nodes. These two networks have low average degrees,
both around 1.3. This is a consequence of the structure of these
networks: there are a small number of &amp;quot;hub&amp;quot; nodes that have links
to a large number of &amp;quot;spoke&amp;quot; nodes, which have few links. As a
result, most of the nodes are &amp;quot;spoke&amp;quot; nodes, usually only having a
single edge. This lack of connections is also shown in another
metric, the &lt;em&gt;graph density&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Graph density is defined as the ratio of the number of edges to
the total possible number of edges between the nodes. The total
possible would be achieved if every node was connected to every
other node. For a graph with &lt;em&gt;n&lt;/em&gt; nodes, this would result in &lt;em&gt;n
choose 2&lt;/em&gt; or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt; edges. The density thus varies from
0 (in a graph with no edges) to 1 (in a graph with every possible
edge). The densities of the financial and programming graphs are
0.01 and 0.04, respectively, so they have low density.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="centrality"&gt;
&lt;h3&gt;Centrality&lt;/h3&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can also calculate metrics which helps us
to better understand the network. One property of nodes in a
network that we are interested in is their centrality. The metric
of &lt;em&gt;betweenness centrality&lt;/em&gt; is one way of calculating this.
The betweenness centrality of a node is the proportion of
shortest paths between any other two nodes that pass through
it. &amp;quot;Spoke&amp;quot; nodes will have low values and &amp;quot;hubs&amp;quot; high values.&lt;/p&gt;
&lt;p&gt;The most central nodes in the financial network are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="46%" /&gt;
&lt;col width="41%" /&gt;
&lt;col width="13%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Betweenness Centrality&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/frugal&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapc&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/collapse&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/gamedeals&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/simpleliving&lt;/td&gt;
&lt;td&gt;0.26&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/canadianhardwareswap&lt;/td&gt;
&lt;td&gt;0.24&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/zerowaste&lt;/td&gt;
&lt;td&gt;0.19&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/meditation&lt;/td&gt;
&lt;td&gt;0.17&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/steam&lt;/td&gt;
&lt;td&gt;0.14&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapcsales&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/financialindependence&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;/r/frugal and /r/buildapc are central because they act as a bridge
between the network's two main branches: one focused on financial
matters and the other focused on computer building and gaming.
Because of this, many shortest paths must pass through them.
/r/frugal also unites the main hubs in the financial branch,
/r/collapse, /r/zerowaste, /r/simpleliving, and
/r/financialindependence.&lt;/p&gt;
&lt;p&gt;/r/collapse is a hub for many small subreddits that are not
linked to any other nodes. Any path from one of these nodes to
another other must necessarily pass through /r/collapse,
contributing to its high centrality.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="clustering"&gt;
&lt;h3&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Another metric for describing a network is the &lt;em&gt;clustering
coefficient.&lt;/em&gt; Before we define this, first define a &lt;em&gt;triangle&lt;/em&gt; as
a sub-graph of three nodes that are all connected to each other.
Suppose we have a node &lt;em&gt;u&lt;/em&gt; with degree &lt;em&gt;n&lt;/em&gt;. The maximum possible
of triangles including &lt;em&gt;u&lt;/em&gt; is &lt;em&gt;n choose 2&lt;/em&gt;, or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt;.
The clustering coefficient is the number of existing triangles
including node &lt;em&gt;u&lt;/em&gt; divided by this maximum possible number.
So, this coefficient will always be between 0 and 1. It can be
interpreted as the tendency of a node to cluster with other nodes.
Any node that is only connected to a single other node will always have
a clustering coefficient of 0. If all of a node's neighboring nodes are
connected, then the node will have a clustering coefficient of 1.&lt;/p&gt;
&lt;p&gt;Most of the nodes in our two networks are spokes connected only to
a single hub node and thus will have a clustering coefficient of
0. Nodes with coefficients significantly larger than 0 are more
rare in these networks. This is perhaps not surprising given that
these are sparse graphs.&lt;/p&gt;
&lt;p&gt;The nodes in the programming network with the highest clustering
coefficients are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="48%" /&gt;
&lt;col width="42%" /&gt;
&lt;col width="10%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Clustering Coefficient&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/programmerhumor&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cseducation&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/computerscience&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cryptocurrencymemes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/compsci&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/freelance&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cs_questions&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/resumes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/coding&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/javascript&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/experienceddevs&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/learnprogramming&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/jobs&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Many of these have only two or three few neighbors, so the
clustering coefficient of 1 is less significant. In contrast,
while /r/csmajors has a coefficient of only 0.17, it has 12
neighbors: out of the 66 possible triangles, 11 of them are fully
connected. This subreddit be part of something closer to a cluster
than many of nodes with a clustering coefficient of 1.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;There is much room for expansion on this sort of analysis. Some
further avenues to explore are:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1. A more extensive network could be constructed by crawling the
actual posts on each messageboard and collecting hyperlinks given
there. Links to webpages outside of reddit.com could also be
crawled.&lt;/p&gt;
&lt;p&gt;2. The number of links between webpages could be tabulated in
order to measure the &lt;em&gt;strength&lt;/em&gt; of each link in the network.&lt;/p&gt;
&lt;p&gt;3. Instead of an undirected graph, the direction of the links
could be incorporated into the model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</content></entry></feed>