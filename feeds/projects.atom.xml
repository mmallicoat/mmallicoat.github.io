<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>▲HEAP - Projects</title><link href="http://marshallmallicoat.com/" rel="alternate"></link><link href="http://marshallmallicoat.com/feeds/projects.atom.xml" rel="self"></link><id>http://marshallmallicoat.com/</id><updated>2019-02-11T00:00:00-06:00</updated><entry><title>Dental Demographics</title><link href="http://marshallmallicoat.com/dental-demographics.html" rel="alternate"></link><published>2018-12-31T00:00:00-06:00</published><updated>2019-02-11T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-12-31:/dental-demographics.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The profitability of a dental practice depends greatly on its
geographic location. The demographics of the patient population
in the area surrounding the practice affects the demand for
dental care (that is, the need and the means to pay). Key
characteristics are the age and income levels of the …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The profitability of a dental practice depends greatly on its
geographic location. The demographics of the patient population
in the area surrounding the practice affects the demand for
dental care (that is, the need and the means to pay). Key
characteristics are the age and income levels of the population.
Another location-dependent factor is the amount of competition
from other dental practices. An underserved area could be a good
place to start a new practice.&lt;/p&gt;
&lt;p&gt;Most dental practices in the United States are privately owned.
A sole dentist or partnership will finance the opening of a new
dental practice themselves. Placing the practice in a location
that will be profitable is an important part of their business
strategy.&lt;/p&gt;
&lt;p&gt;This is a concern even for dentists who do not own their own
practice. Dentists in the US are usually paid as a percent of the
amount of dental care they provide, which in turn is dependent of
the amount of business the practice can bring in.&lt;/p&gt;
&lt;p&gt;In order to find suitable locations for a new dental practice, I
compiled demographic data from the US Census Bureau and records of
existing practices from online business directories. I confined
my search in the states of Kansas and Missouri, with a particular
focus on the metropolitan area of Kansas City, Missouri.&lt;/p&gt;
&lt;p&gt;All of the code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/dental-demographics"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="american-community-survey"&gt;
&lt;h2&gt;American Community Survey&lt;/h2&gt;
&lt;p&gt;I used demographic data from the &lt;a class="reference external" href="https://www.census.gov/programs-surveys/acs.html"&gt;American Community Survey&lt;/a&gt; (ACS)
published by the US Census Bureau. This survey contains data from
a five-year period, 2012–2016. The attributes collected include
age, sex, race, occupation, veteran status, insurance benefits,
and many others.&lt;/p&gt;
&lt;p&gt;The ACS has some advantages over the &lt;a class="reference external" href="https://www.census.gov/programs-surveys/decennial-census.html"&gt;Decennial Census&lt;/a&gt;. It contains
many more attributes than the Census. The data is also aggregated
and published annually rather than every ten years, providing a
more up-to-date snapshot. One drawback is that the ACS only covers
a sample of the population rather than the population entire.&lt;/p&gt;
&lt;p&gt;In the ACS, the finest granularity of the geographic component
is the Public Use Microdata Area (PUMA). These are generally
small geographic regions, containing no fewer than 100,000 people
(in order to ensure anonymity and statistical credibility). The
boundaries of the PUMAs do not cross state or county lines, so the
data can be aggregated at that level. The boundaries also do not
cross the Census Blocks and Census Tracts used in the Decennial
Census. &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="geographic hierarchy in data" src="./figures/data-geo-hierarchy.jpg" /&gt;
&lt;p class="caption"&gt;The PUMA falls between the County and Census Tract granularities.&lt;/p&gt;
&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;They do, however, cross the boundaries of ZIP codes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="age-and-income"&gt;
&lt;h2&gt;Age and Income&lt;/h2&gt;
&lt;p&gt;Areas with higher incomes will spend more on dental care than
those with lower incomes. This is not surprising, especially in
the US where almost all dental care is paid for out-of-pocket or
by private insurance. &lt;a class="footnote-reference" href="#id6" id="id4"&gt;[2]&lt;/a&gt; Areas with older populations will spend
more on dental care, because older people tend to have more dental
problems and also because of the positive correlation between age
and income/wealth. So, we will focus on these two demographic
attributes in considering a location.&lt;/p&gt;
&lt;p&gt;Below is snippet of the statistics for each PUMA in Kansas (state
code 20 &lt;a class="footnote-reference" href="#id7" id="id5"&gt;[3]&lt;/a&gt;) and Missouri (state code 29).&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="7%" /&gt;
&lt;col width="6%" /&gt;
&lt;col width="13%" /&gt;
&lt;col width="29%" /&gt;
&lt;col width="13%" /&gt;
&lt;col width="30%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;State&lt;/th&gt;
&lt;th class="head"&gt;PUMA&lt;/th&gt;
&lt;th class="head"&gt;Population&lt;/th&gt;
&lt;th class="head"&gt;Median Household Income
(2016 Dollars)&lt;/th&gt;
&lt;th class="head"&gt;Median Age&lt;/th&gt;
&lt;th class="head"&gt;Percent Aged 60 or Older&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;109,867&lt;/td&gt;
&lt;td&gt;$41,200&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;26.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;147,564&lt;/td&gt;
&lt;td&gt;$42,318&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;25.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;134,983&lt;/td&gt;
&lt;td&gt;$32,714&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;12.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;122,120&lt;/td&gt;
&lt;td&gt;$42,564&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;20.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;161,762&lt;/td&gt;
&lt;td&gt;$40,129&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;16.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;116,104&lt;/td&gt;
&lt;td&gt;$81,513&lt;/td&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;td&gt;18.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;602&lt;/td&gt;
&lt;td&gt;153,179&lt;/td&gt;
&lt;td&gt;$65,904&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;22.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;603&lt;/td&gt;
&lt;td&gt;158,524&lt;/td&gt;
&lt;td&gt;$69,349&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;13.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;144,839&lt;/td&gt;
&lt;td&gt;$106,734&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;td&gt;18.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;700&lt;/td&gt;
&lt;td&gt;116,206&lt;/td&gt;
&lt;td&gt;$26,172&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;15.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I also visualized these results using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/QGIS"&gt;QGIS&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Median Household Income" src="./figures/median-household-income.png" /&gt;
&lt;p class="caption"&gt;Higher median household incomes are shown in darker shades.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see that the highest income areas in these two states are
in the suburbs of Kansas City and Saint Louis, on the western
and eastern borders of Missouri. There are also slightly higher
incomes in the suburbs of Topeka (in north–central Kansas) and
Wichita (in south–central Kansas).&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Median Age" src="./figures/median-age.png" /&gt;
&lt;p class="caption"&gt;Higher median ages are shown in darker shades.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The median age is generally higher in more rural regions. In the
areas where large universities are located, we see a much lower
median age than elsewhere. The youthful patches on the map can be
explained by Kansas State University in Manhattan, the University
of Kansas in Lawrence, the University of Missouri in Columbia, and
Missouri State University in Springfield.&lt;/p&gt;
&lt;p&gt;An outlier in income and age is southwest Kansas: it has unusually
high incomes and unusually young population for a rural area.
My guess is that this is due to oil and gas exploitation in the
region, attracting young workers and paying relatively high wages.
See the chart from geological survey below.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="KS Geological Survey" src="./figures/ks_geological_survey_fig3.jpg" /&gt;
&lt;p class="caption"&gt;&lt;a class="reference external" href="http://www.kgs.ku.edu/Publications/PIC/pic32.html"&gt;Kansas Geological Survey, Public Information Circular (PIC) 32, December 2011&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The existing socialized medical programs in the US (e.g.,
Medicaid and Medicare) generally do not provide dental coverage.
One exception is Tricare, which does.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The codes used in the ACS data accord with the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code"&gt;Federal Information Processing Standard&lt;/a&gt;
(FIPS) state codes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="competition"&gt;
&lt;h2&gt;Competition&lt;/h2&gt;
&lt;p&gt;A third consideration is the number of dental practices already
operating the region. The more dental practices there are, the
more competition. This leads to lower utilization of the capacity
of each dental practice and lower profitability.&lt;/p&gt;
&lt;p&gt;To assess this, I scraped the listings of dentists and dental
practices from an online business directory. Since I wanted to
focus on the metropolitan area of Kansas City, I collected listing
from the region around Lenexa, Kansas, an inner-ring suburb. &lt;a class="footnote-reference" href="#id9" id="id8"&gt;[4]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Given the address of each dental practice, I used the &lt;a class="reference external" href="https://www.openstreetmap.org"&gt;Open Street
Map&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstreetmap.org/wiki/Nominatim"&gt;Nominatim&lt;/a&gt; API to geocode each location, looking up the
latitude and longitude coordinates for each street address.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Shapefile"&gt;shapefiles&lt;/a&gt; provided by the Census Bureau for the PUMAs
contain the boundaries of these geographic regions. Using the
&lt;tt class="docutils literal"&gt;fiona&lt;/tt&gt; Python library, we can easily open and manipulate
shapefiles. In conjunction with the &lt;tt class="docutils literal"&gt;shapely&lt;/tt&gt; library, we can
find which PUMA each of the dental practices is located in, by way
of its coordinates.&lt;/p&gt;
&lt;p&gt;After removing duplicate locations from our list of dental
practices, we can then tabulate the number in practices in each
PUMA. One rule of thumb is that an ideal practice has 2000 active
patients. So, we would be looking for areas with around 2000 or
more people per practice.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Population per Dental Practice" src="./figures/practice-count.png" /&gt;
&lt;p class="caption"&gt;Population per dental practice in the Kansas City area&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;According to our chart, southwest Johnson County (bottom left
PUMA) and western Kansas City (two PUMAs on right) have a suitable
ratio. Wyandotte County (top center PUMA) is somewhat underserved,
so it could be a profitable location for a practice. Southeast
Johnson County (bottom center PUMA) has the lowest ratio; however,
it is also the area with the highest median household income
Kansas or Missouri, so it may nevertheless be a good location.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id9" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id8"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Since I did not scrape the entire business directory, only the
data for the areas close to the origin of Lenexa, KS, can be
expected to be reasonably complete.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="future-directions"&gt;
&lt;h2&gt;Future Directions&lt;/h2&gt;
&lt;p&gt;The attributes considered here may be a good start, but there are
other factors worth looking at. Future analysis could investigate
the dental insurance providers in the area and how much they will
pay for various dental procedures. The same procedure in one area
may be better compensated than in another due to the typical
insurance coverage of the patients.&lt;/p&gt;
&lt;p&gt;The data quality of the existing dental practices could be
improved. The addresses scraped from the business directory were
quite dirty, having problems like:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Some dentists are listed multiple times at different practices&lt;/li&gt;
&lt;li&gt;Some dental practices listed are no longer operating&lt;/li&gt;
&lt;li&gt;Errors in addresses for practices, such as the wrong city or ZIP code given&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also little trust in the listings being complete. In the
future, there may be better sources for this information, such as
lists of in-network dental practitioners published by insurance
companies.&lt;/p&gt;
&lt;p&gt;Open Street Map's Nominatim geocoding API seems to be less
tolerant to malformed addresses than Google Maps'. For a
commercial application, it would probably be worth paying for
Google's service.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Does the Computer Cheat in Mario Party?</title><link href="http://marshallmallicoat.com/mario-party.html" rel="alternate"></link><published>2018-11-17T00:00:00-06:00</published><updated>2018-11-18T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-17:/mario-party.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I happened to play the video game &lt;em&gt;Mario Party 5&lt;/em&gt; with my brother
over a holiday. The game involves rolling dice to move your
character around a playing board. Since we seemed to be getting
lower rolls on average than the computer players, we wondered if
perhaps they were …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I happened to play the video game &lt;em&gt;Mario Party 5&lt;/em&gt; with my brother
over a holiday. The game involves rolling dice to move your
character around a playing board. Since we seemed to be getting
lower rolls on average than the computer players, we wondered if
perhaps they were given an advantage somehow. I manually recorded
a sample of the dice rolls from the game in order to test this
possibility.&lt;/p&gt;
&lt;p&gt;The data and code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/mario-party"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;!-- Style: Student's *t*-test; Mann–Whitney *U* test; Wilcoxon rank-sum test; (Pearson's) chi-squared test; Smirnov–Kolmogorov test; *p*-value --&gt;
&lt;/div&gt;
&lt;div class="section" id="distribution-of-dice-rolls"&gt;
&lt;h2&gt;Distribution of Dice Rolls&lt;/h2&gt;
&lt;p&gt;Each turn, a player rolls a die which returns an integer value
from 1 to 10. Naively, we would assume that the die behaves like
physical dice: that is, that each outcome is equally likely. &lt;a class="footnote-reference" href="#id3" id="id1"&gt;[1]&lt;/a&gt;
The chart below shows the frequencies of the outcomes for the
computer and human players from my sample.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of dice roll frequencies" src="./figures/dice-outcome-frequencies.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Simply looking at the frequencies, it does seem that the computer
players tend to get more very high rolls (9s and 10s) than the
human players and that the human player get more very low rolls
(1s and 2s), but we must do a statistical test to make a rigorous
assessment. We need to perform a goodness of fit test to determine
if our sample likely comes from the assumed distribution: a
discrete uniform distribution with support on [1, 10].&lt;/p&gt;
&lt;p&gt;A simple and effective goodness of fit test is the chi-squared
test. &lt;a class="footnote-reference" href="#id4" id="id2"&gt;[2]&lt;/a&gt; This compares the frequencies for each outcome to the
expected frequency under the null hypothesis that the outcomes are
equally likely. Performing the test, we get the following results:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%" /&gt;
&lt;col width="39%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Player&lt;/th&gt;
&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Human&lt;/td&gt;
&lt;td&gt;6.000&lt;/td&gt;
&lt;td&gt;0.740&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Computer&lt;/td&gt;
&lt;td&gt;5.973&lt;/td&gt;
&lt;td&gt;0.743&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-values are much higher than any reasonable significance
level, so we cannot reject the assumption that the dice is fair
for both the computer and human players.&lt;/p&gt;
&lt;p&gt;Since our sample size is somewhat small, around 40 samples for
each type of player, the expected frequency is around 4 for each
possible outcome. This is less than the rule of thumb that you
should have a minimum expectation of 5 in each category; so,
our test results may not be reliable. We can resolve this by
instead &lt;em&gt;pooling&lt;/em&gt; the results into five categories of outcomes:
rolls of 1 or 2, 3 or 4, 5 or 6, 7 or 8, and 9 or 10. This gives
an expectation for each category of about 8, which should be
sufficient. The new results are then:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%" /&gt;
&lt;col width="39%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Player&lt;/th&gt;
&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Human&lt;/td&gt;
&lt;td&gt;5.500&lt;/td&gt;
&lt;td&gt;0.240&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Computer&lt;/td&gt;
&lt;td&gt;4.757&lt;/td&gt;
&lt;td&gt;0.313&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-values are lower than previously, but the test is still
inconclusive. Both dice appear to be fair.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Additionally, we would expect each roll to be independent of
the others, but I have not tested this hypothesis.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I had first attempted to use the exact test of goodness of
fit. This requires evaluating the CDF of a multinomial
distribution. The CDF of the multinomial distribution
is simply the summation of the pmf evaluated at the
appropriate subset of the sample space. The size of the
sample space is given by a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)#Theorem_two"&gt;theorem in combinatorics&lt;/a&gt;. The number of ways of arranging &lt;em&gt;n&lt;/em&gt; objects into
&lt;em&gt;k&lt;/em&gt; ordered partitions is &lt;em&gt;n + k - 1 choose n&lt;/em&gt;. So, for a
10-dimensional multinomial distribution and a sample size of 40,
this is &lt;em&gt;49 choose 40&lt;/em&gt;, or 2,054,455,634 combinations. To evaluate
the CDF at a particular value, a program would have to iterate
through a loop as many as 2 billion times, calculating the pmf
of one combination each time. Even using the &lt;tt class="docutils literal"&gt;multiprocessing&lt;/tt&gt;
library to take advantage of the two processor cores on my laptop,
I estimate this would take 5.5 hours. I don't think the additional
accuracy of the exact test over the chi-squared test is worth the
computational time.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="mean-dice-roll"&gt;
&lt;h2&gt;Mean Dice Roll&lt;/h2&gt;
&lt;p&gt;Although we are unable to discern that the dice are anything but
fair, the mean dice roll for the computer players is 5.14, which
is slightly higher than the mean for the human players of 4.83.
Even if the outcomes of the dice rolls are close enough to fair
to avoid detection, there might still be some edge give to the
computer player. Instead of testing the distribution, we can
directly test whether the means of the two dice rolls are equal.&lt;/p&gt;
&lt;p&gt;A common test of the means of two population is the Student's
&lt;em&gt;t&lt;/em&gt;-test. This tests whether the difference of the means of the
two populations is significantly different than zero. The null
hypothesis is that the expected difference is exactly zero.&lt;/p&gt;
&lt;p&gt;The results of this test show:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%" /&gt;
&lt;col width="48%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;0.496&lt;/td&gt;
&lt;td&gt;0.621&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this large &lt;em&gt;p&lt;/em&gt;-value, we cannot reject the null hypothesis
that the means are equal.&lt;/p&gt;
&lt;p&gt;The Student's &lt;em&gt;t&lt;/em&gt;-test assumes that the means of the samples are
normally distributed. This will be true asymptotically, proven
in the Central Limit Theorem. &lt;a class="footnote-reference" href="#id6" id="id5"&gt;[3]&lt;/a&gt; Nevertheless, there exists
a nonparametric test which is almost as powerful as Student's
&lt;em&gt;t&lt;/em&gt;-test in many contexts: the Mann–Whitney &lt;em&gt;U&lt;/em&gt; test (also known
as the Wilcoxon sum-rank test). This tests the null hypothesis
that a random sample from one population is equally likely to
be greater than or less than a random sample from a second
population.&lt;/p&gt;
&lt;p&gt;The results of the Mann–Whitney &lt;em&gt;U&lt;/em&gt; test are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%" /&gt;
&lt;col width="48%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;783.000&lt;/td&gt;
&lt;td&gt;0.663&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Again, these are inconclusive. We cannot reject the hypothesis
that the means are equal.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We have already assumed that the samples are independent,
but for the Central Limit Theorem to hold, the distribution of the
populations must also have finite variances. Since the outcomes of
the dice rolls are confined to the range [1, 10], this will be the
case.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="power-of-statistical-tests"&gt;
&lt;h2&gt;Power of Statistical Tests&lt;/h2&gt;
&lt;!-- Alternate title: Power Analysis --&gt;
&lt;p&gt;It is possible to computer the &lt;em&gt;power&lt;/em&gt; of a statistical test: the
probability of rejecting the null hypothesis given that the null
hypothesis is actually false. The power of a test is a function of
the sample size and the value of the distribution parameters under
the alternate hypothesis. By computing the power of a test, we can
get a sense of if our sample size is sufficiently large to perform
a maximally powerful test.&lt;/p&gt;
&lt;p&gt;Below is a plot of the power curve of Student's &lt;em&gt;t&lt;/em&gt;-test for
various sample sizes.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of power curve for Student's t-test" src="./figures/students-t-power-curve.png" /&gt;
&lt;/div&gt;
&lt;p&gt;We can see that, with a sample size of 77, our test is very
close the power curve of a test with a sample size of 1000.
Increasing the sample size of our test would only marginal
increase the power. Our sample size is large enough for performing
the Student's &lt;em&gt;t&lt;/em&gt;-test at close to its maximum power. &lt;a class="footnote-reference" href="#id8" id="id7"&gt;[4]&lt;/a&gt;&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id8" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Student's &lt;em&gt;t&lt;/em&gt; distribution is asymptotically normal as the
number of the degree of freedom approaches infinity. This is why we
see convergence in the power curves to some upper limit.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- TODO: Plot power curve of chi-squared test as well --&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It seems that the dice in this game are fair and that no advantage
is given to either the computer or human players. Or, if either
of these is not the case, the deviation is small enough that our
tests are not powerful enough to detect it.&lt;/p&gt;
&lt;p&gt;However, there are additional ways that the game may still bend
the rules that we have not tested. For example, if a player
had a run of low rolls, the game might increase the probably
of a higher rolls subsequently, in order to make the game more
forgiving. We could test the assumption that the rolls are indeed
independent of each other. There exist many &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Randomness_tests"&gt;tests of randomness&lt;/a&gt; which can
discern &amp;quot;non-randomness&amp;quot; in a sample, even if the frequencies
of its outcomes match exactly the expected proportions.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Predicting House Prices</title><link href="http://marshallmallicoat.com/kaggle-house-prices.html" rel="alternate"></link><published>2018-11-12T00:00:00-06:00</published><updated>2018-11-12T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-12:/kaggle-house-prices.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms, the size of the yard, and so on.
The data are from houses in Ames, Iowa, compiled for use in data
science education.&lt;/p&gt;
&lt;p&gt;To solve the problem, I developed a Generalized Linear Regression
(GLM) model. The GLM model works by fitting a function to the
features of houses with known prices; then, to predict the price
of an additional house, the estimated function is evaluated with
the house's particular features. The function takes the form of a
&lt;em&gt;hyperplane&lt;/em&gt; (a generalization of the plane to higher dimensional
space). Usually the response variable, in our case the house
price, is transformed before the fitting takes place.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/kaggle-house-prices"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="data-prep"&gt;
&lt;h2&gt;Data Prep&lt;/h2&gt;
&lt;p&gt;The first step I took was to stratify the data provided,
complete with the price of each house, into training and
cross-validation (CV) datasets. Having a CV dataset lets you
compare the performance of models on out-of-sample data, thereby
avoiding overfitting and getting a more accurate estimate of its
performance. I randomly partitioned the labeled dataset into the
training and CV datasets, since I do not know if there is some
order to how they are presented in the file Kaggle provides which
might bias my model.&lt;/p&gt;
&lt;p&gt;Next, I dealt with any missing values in the dataset. For numeric
variables, I calculated the mean of the variable in the training
data and substituted this value for any missing in the training
and CV datasets. For the categorical variables, I substituted a
new value &amp;quot;Unknown.&amp;quot;&lt;/p&gt;
&lt;p&gt;To select from the many features, I used some simple heuristics.
For the numeric variables, I calculated the variance of each
within the training data. I produced a scatter plot of the
variances of the variables, sorted in ascending order. In this
plot, I found an &amp;quot;elbow&amp;quot; where there was a significant drop-off
in variance. I then selected all variables with variance above
this threshold. This amounted to 12 variables, about a third of
the numeric variables available. The rationale behind this is that
variables with low variance do not provide much discriminating
information between houses, since all of the houses will have
similar values.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of variances of numeric variables" src="./figures/numeric-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the plot of variances of numeric variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For each of the categorical variables, I calculated the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;entropy&lt;/a&gt;,
assuming each value was pulled from a multinomial distribution.
Entropy is a measure of the amount of &amp;quot;information&amp;quot; contained in
a stochastic process. Random variables with little &amp;quot;surprise&amp;quot; in
their realized values will have low entropy. For binary variables,
the entropy calculated is equivalent to the variance of the
corresponding Bernoulli distribution. After calculating the
entropy, I plotted a histogram, found an &amp;quot;elbow&amp;quot; is use for the
threshold, and selected all of the variables above this threshold,
in the same manner as the numeric variables. This amounted to 14
variables, about a third of the categorical variables available.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of entropy of categorical variables" src="./figures/categorical-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the histogram of entropy of categorical variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The selected categorical variables were then encoded as dummy
variables, &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt; so that they can be included in the regression.
One trip-up was that there are values of categorical variables
appearing in the test data that do not appear in the training/CV
data. The universe of values must be known ahead of time in order
to encode the variable as dummy variables. Given a new dataset
containing unseen values of a categorical variable, the model
could not be applied.&lt;/p&gt;
&lt;p&gt;It is somewhat challenging to develop a processing pipeline
that can be applied to all datasets uniformly, particularly
when the processing procedure is &amp;quot;fitted&amp;quot; to the training data.
The processing must then be applied to training data first, the
parameters estimated, and those parameters stored somewhere so
that they can be applied when processing the CV and testing
datasets. Instead of writing out the parameters to disk, as I did
for the scaler and regression parameters, I simply kept them in
memory and processed all of the datasets at once. This is less
than ideal, since I wouldn't have the parameters readily available
to apply to a new dataset, which would be necessary if this model
were used in an application.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Dummy variables are a collection of binary variables whose
combination correspond to one of the values of the categorical
variable. The simplest example is a variable with possible values
&amp;quot;Male&amp;quot; and &amp;quot;Female&amp;quot; being encoded as 1 and 0. For variables with
&lt;em&gt;n&lt;/em&gt; possible values, &lt;em&gt;n - 1&lt;/em&gt; dummy variables are required.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="training"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;Before training the model, I performed some transformations on
the data. I standardized the features, subtracting the mean and
dividing by standard deviation to create features with zero mean
and unit variance. If the features have different scales, the
magnitude of the fitted coefficients in the linear model will be
influenced by the scale of the underlying variables and harder to
compare. Coefficients of variables with a larger scale would also
be penalized more highly if regularization is applied.&lt;/p&gt;
&lt;p&gt;Another transformation was to take the logarithm of the sale
price response variable. There are two reasons for this: First,
like most currency values, the house prices in the data are not
normally distributed, which violates an assumption of the linear
regression. This can be seen in the histogram below over which
I've overlaid a fitted normal distribution.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of house prices" src="./figures/y-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;By log-transforming the response variable, it is much closer to
following a normal distribution. &lt;a class="footnote-reference" href="#id4" id="id3"&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of log-transformed house prices" src="./figures/y-transformed-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Secondly, the loss function specified for the Kaggle competition
is the mean squared error of the &lt;em&gt;logarithm&lt;/em&gt; of the house prices
predicted. If we wish to develop a model that performs well under
this loss function, we must optimize the parameters of our model
with respect to it.&lt;/p&gt;
&lt;p&gt;My initial model resulted in some very large positive and negative
coefficients in the fitted model. Due to the limited precision
of floating point arithmetic, these coefficients lead to some
overflows and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow"&gt;underflows&lt;/a&gt;, respectively, in the predicted value of
the response variable. To remedy this, I used a ridge regression
instead, which adds a regularization term to the loss function
used for fitting the model, thereby penalizing coefficients with
large magnitude. This solved the problem of unreasonable large
coefficients.&lt;/p&gt;
&lt;p&gt;After training the model, I saved the parameters of both the
standardization procedure and the linear regression. These are
both are needed in order to repeat the preprocessing steps and
make predictions from the CV and test datasets.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Processes that are the sum of many independent occurrences
generally follow a normal distribution, which is consistence with
the Central Limit Theorem. An example of this is human height,
which is perhaps the result of the expression of many different
genes, the quality of nutrition through each phase of childhood,
the effects of childhood disease, etc., which are generally
independence events, each having a small effect. Processes like
prices or salaries cannot be normally distributed on the face
since they cannot have negative values. Secondly, instead of the
constituents having an additive effect, they seem to have more of
a &lt;em&gt;multiplicative&lt;/em&gt; effect on the outcome. Learning two new skills
will increase your salary more than that sum of each alone.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="prediction"&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;p&gt;To make predictions given the CV and test datasets, the
preprocessing steps and repeated:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Standardize the variables using means and standard deviations
from training dataset&lt;/li&gt;
&lt;li&gt;For the CV dataset, log-transform the response variable. (We do
not know the value of the response variable for the testing data,
of course.)&lt;/li&gt;
&lt;li&gt;Apply our regression model to make a prediction: multiply
values of the features by the fitted coefficients, sum these up,
and add the intercept.&lt;/li&gt;
&lt;li&gt;For the CV dataset, calculate the value of the loss function as
a diagnostic.&lt;/li&gt;
&lt;li&gt;Before writing out the predictions, reverse the log-transform
by exponentiating the predicted value.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Kaggle competition is judged by the square root of the mean
squared error (RMSE) of the predictions of the log-transformed
house prices. This metric for our model (on the test dataset) is
0.168, which is fairly middling compared to the leaderboard on the
Kaggle website. For the CV dataset, the metric is 0.166, which is
close to that of the test dataset, as we would expect.&lt;/p&gt;
&lt;p&gt;The metric is somewhat difficult to interpret, so I calculated the
RMSE of the &lt;em&gt;un&lt;/em&gt;-transformed prices for comparison. The RMSE for
the untransformed prices in the CV dataset is $37,576. This is
very roughly &lt;a class="footnote-reference" href="#id6" id="id5"&gt;[3]&lt;/a&gt; the expected deviation of our prediction from
the true price. The mean house price in this dataset is $178,186;
so, although our error is significant, the predictions are within
the ballpark of the true values.&lt;/p&gt;
&lt;p&gt;There are many avenues to explore which could improve the
model's performance. Here are some things to try in the future:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Engineer some custom features, especially ones that capture
interactions between variables. These might be something like the
ratio of bathrooms to bedrooms, or ratio of plot area to house
floor area.&lt;/li&gt;
&lt;li&gt;Make use of the ordinal variables: there are some variables that
are actually ordinal, not categorical. An example of this is X.
Instead of ignoring the ordering of the levels of the variable,
they could be taken advantage of.&lt;/li&gt;
&lt;li&gt;Try some alternate models, especially those that can fit
non-linear functions. There may be some non-linear interactions
between the house price and the independent variables, such as
the price not being monotonically increasing with the value of an
independence variable. One plausible explanation of this might be
something along the lines of: a larger yard may correlate with a
more valuable property, but it may correlate with a more rural
location; the negative effect of the rural location on the house
price might outweigh the increase from the larger yard.&lt;/li&gt;
&lt;li&gt;Supplement external data: we are given the names of
neighborhoods of the houses. There is publicly available data on
houses and their prices from these locations. This data could be
collected and used to supplement the data provided by Kaggle. Or,
a secondary model could be built from the external data and then
combined with the model trained on the Kaggle data in an ensemble.&lt;/li&gt;
&lt;/ul&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The RMSE is in fact the standard deviation of the
residuals, which are the differences between each prediction and
true value. The standard deviation is the square root of the
expected squared deviation, rather than the expected deviation.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content></entry><entry><title>Analyzing Subreddit Networks</title><link href="http://marshallmallicoat.com/subreddit-networks.html" rel="alternate"></link><published>2018-11-02T00:00:00-05:00</published><updated>2019-02-07T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-02:/subreddit-networks.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form a
network of closely related online communities. Because these
lists of links are human-curated, they give high-quality
indicators of related topics and communities. By following these
links programmatically, we can collect the data needed to to
visualize and analyze these networks.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/subreddit-graphs"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="scrape-the-data"&gt;
&lt;h2&gt;Scrape the Data&lt;/h2&gt;
&lt;p&gt;I wrote simple web scraper using the Python library &lt;tt class="docutils literal"&gt;scrapy&lt;/tt&gt;
which collects the list of links and other information from each
subreddit page. Starting from an initial subreddit, the scraper
searches the sidebar on the page for any related subreddits that
have been linked there. The scraper then follows those links and
iteratively searches for more links. The name of each subreddit is
collected, along with its description, number of subscribers, and
the list of links to other subreddits.&lt;/p&gt;
&lt;p&gt;From these data, it is possible to construct the network of
subreddits surrounding the initial page. In this network (often
called a &lt;em&gt;graph&lt;/em&gt; in mathematics), each node is a subreddit and
each edge is a hyperlink between them. For simplicity, I decided
to build a non-directed graph, ignoring the directionality of
hyperlinks: a hyperlink from subreddit A to B is treated in the
same manner as a hyperlink from subreddit B to A.&lt;/p&gt;
&lt;p&gt;The scraper uses CSS selectors to locate the desired elements on
each webpage. Here is a code snippet with the selectors:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sub_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subscriber_conversion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-12::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# convert from string to numeric&lt;/span&gt;
    &lt;span class="n"&gt;links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0 a::attr(href)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="s1"&gt;'subreddit'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'description'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-14::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'links'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="s1"&gt;'subscribers'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sub_count&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The sidebar on the page is identified using the method
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;response.css('div.s1s8pi67-0')&lt;/span&gt;&lt;/tt&gt; where &lt;tt class="docutils literal"&gt;response&lt;/tt&gt; is the
object representing the returned webpage. The string
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;s1s8pi67-0&lt;/span&gt;&lt;/tt&gt; is a unique class of the &lt;tt class="docutils literal"&gt;div&lt;/tt&gt; element containing
the sidebar. &lt;a class="footnote-reference" href="#selectors" id="id1"&gt;[1]&lt;/a&gt; The scaper returns the name of the
subreddit, the subreddit's description, the list of links in the
sidebar, and the number of subscribers.&lt;/p&gt;
&lt;p&gt;I chose two subreddits to use as the starting points to crawl
through related pages, which results in two networks for analysis:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;one centered at /r/programming, which I will refer
to as the &amp;quot;programming network&amp;quot;&lt;/li&gt;
&lt;li&gt;one centered at /r/financialindependence, which I
will refer to as the &amp;quot;financial network.&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these initial subreddits have a fair number of links
listed in their sidebars, which should lead to larger, more
complex networks.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="selectors" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Using these seemingly random attribute values for
the selectors is less than ideal: they are non-semantic and they
seem to change fairly frequently, possibly with each build of the
website. An improvement woudl be to find a more robust selector. I
think it would be possible to use an XPath selector to find the
text &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/r/[subreddit&lt;/span&gt; name]&lt;/tt&gt; that appears in the sidebar, and then
select the document element containing this a few steps up the
hierarchy.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="visualizations"&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;p&gt;After collecting the network data, we can use the library
&lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt; to visualize and analyze the networks. I quickly
made a couple plots using &lt;tt class="docutils literal"&gt;matplotlib&lt;/tt&gt; to visualize the graphs.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="network centered as /r/programming" src="./figures/prog-graph.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Unfortunately, these were difficult to read and not very useful
for exploring the networks. To remedy this, I decided to use the
visualization library &lt;tt class="docutils literal"&gt;d3&lt;/tt&gt; (written in JavaScript) to make some
interactive plots. After we convert the network data into the
&amp;quot;node-link&amp;quot; JSON format using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can read it into a
HTML file containing JavaScript visualizations.&lt;/p&gt;
&lt;p&gt;Click on the images below to view the interactive plots.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/fin-force.html"&gt;&lt;img alt="network centered as /r/financialindependence" src="./figures/fin-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/financialindependence&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/prog-force.html"&gt;&lt;img alt="network centered as /r/programming" src="./figures/prog-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/programming&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In these charts, the relative number of subscribers to each
subreddit is represented by the radius of the node (using a
log-scale).&lt;/p&gt;
&lt;p&gt;One of the most salient features is the &amp;quot;spoke-and-hub&amp;quot; structure:
a larger subreddit links to many smaller subreddits, which
are often dedicated to a more specific topic. For example,
/r/financialindependence is linked to country-specific subreddits
for Canada (/r/personalfinancecanada), UK (/r/ukpersonalfinance),
and so on. These can often be sensibly grouped into a cluster of
nodes based on their subject matter. An example of this is the
closely related subreddits surrounding /r/collapse which are all
dedicated to the topic of societal and economic collapse.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis"&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;div class="section" id="average-degree-and-density"&gt;
&lt;h3&gt;Average Degree and Density&lt;/h3&gt;
&lt;p&gt;One descriptive statistic of a graph is the &lt;em&gt;average degree&lt;/em&gt;. The
degree of a node is the number of edges connected to it. The
average degree of a graph is simply the average of the degrees of
each of its nodes. These two networks have low average degrees,
both around 1.3. This is a consequence of the structure of these
networks: there are a small number of &amp;quot;hub&amp;quot; nodes that have links
to a large number of &amp;quot;spoke&amp;quot; nodes, which have few links. As a
result, most of the nodes are &amp;quot;spoke&amp;quot; nodes, usually only having a
single edge. This lack of connections is also shown in another
metric, the &lt;em&gt;graph density&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Graph density is defined as the ratio of the number of edges to
the total possible number of edges between the nodes. The total
possible would be achieved if every node was connected to every
other node. For a graph with &lt;em&gt;n&lt;/em&gt; nodes, this would result in &lt;em&gt;n
choose 2&lt;/em&gt; or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt; edges. The density thus varies from
0 (in a graph with no edges) to 1 (in a graph with every possible
edge). The densities of the financial and programming graphs are
0.01 and 0.04, respectively, so they have low density.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="centrality"&gt;
&lt;h3&gt;Centrality&lt;/h3&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can also calculate metrics which helps us
to better understand the network. One property of nodes in a
network that we are interested in is their centrality. The metric
of &lt;em&gt;betweenness centrality&lt;/em&gt; is one way of calculating this.
The betweenness centrality of a node is the proportion of
shortest paths between any other two nodes that pass through
it. &amp;quot;Spoke&amp;quot; nodes will have low values and &amp;quot;hubs&amp;quot; high values.&lt;/p&gt;
&lt;p&gt;The most central nodes in the financial network are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="46%" /&gt;
&lt;col width="41%" /&gt;
&lt;col width="13%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Betweenness Centrality&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/frugal&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapc&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/collapse&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/gamedeals&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/simpleliving&lt;/td&gt;
&lt;td&gt;0.26&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/canadianhardwareswap&lt;/td&gt;
&lt;td&gt;0.24&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/zerowaste&lt;/td&gt;
&lt;td&gt;0.19&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/meditation&lt;/td&gt;
&lt;td&gt;0.17&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/steam&lt;/td&gt;
&lt;td&gt;0.14&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapcsales&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/financialindependence&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;/r/frugal and /r/buildapc are central because they act as a bridge
between the network's two main branches: one focused on financial
matters and the other focused on computer building and gaming.
Because of this, many shortest paths must pass through them.
/r/frugal also unites the main hubs in the financial branch,
/r/collapse, /r/zerowaste, /r/simpleliving, and
/r/financialindependence.&lt;/p&gt;
&lt;p&gt;/r/collapse is a hub for many small subreddits that are not
linked to any other nodes. Any path from one of these nodes to
another other must necessarily pass through /r/collapse,
contributing to its high centrality.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="clustering"&gt;
&lt;h3&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Another metric for describing a network is the &lt;em&gt;clustering
coefficient.&lt;/em&gt; Before we define this, first define a &lt;em&gt;triangle&lt;/em&gt; as
a sub-graph of three nodes that are all connected to each other.
Suppose we have a node &lt;em&gt;u&lt;/em&gt; with degree &lt;em&gt;n&lt;/em&gt;. The maximum possible
of triangles including &lt;em&gt;u&lt;/em&gt; is &lt;em&gt;n choose 2&lt;/em&gt;, or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt;.
The clustering coefficient is the number of existing triangles
including node &lt;em&gt;u&lt;/em&gt; divided by this maximum possible number.
So, this coefficient will always be between 0 and 1. It can be
interpreted as the tendency of a node to cluster with other nodes.
Any node that is only connected to a single other node will always have
a clustering coefficient of 0. If all of a node's neighboring nodes are
connected, then the node will have a clustering coefficient of 1.&lt;/p&gt;
&lt;p&gt;Most of the nodes in our two networks are spokes connected only to
a single hub node and thus will have a clustering coefficient of
0. Nodes with coefficients significantly larger than 0 are more
rare in these networks. This is perhaps not surprising given that
these are sparse graphs.&lt;/p&gt;
&lt;p&gt;The nodes in the programming network with the highest clustering
coefficients are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="48%" /&gt;
&lt;col width="42%" /&gt;
&lt;col width="10%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Clustering Coefficient&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/programmerhumor&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cseducation&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/computerscience&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cryptocurrencymemes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/compsci&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/freelance&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cs_questions&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/resumes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/coding&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/javascript&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/experienceddevs&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/learnprogramming&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/jobs&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Many of these have only two or three few neighbors, so the
clustering coefficient of 1 is less significant. In contrast,
while /r/csmajors has a coefficient of only 0.17, it has 12
neighbors: out of the 66 possible triangles, 11 of them are fully
connected. This subreddit be part of something closer to a cluster
than many of nodes with a clustering coefficient of 1.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;There is much room for expansion on this sort of analysis. Some
further avenues to explore are:&lt;/p&gt;
&lt;p&gt;1. A more extensive network could be constructed by crawling the
actual posts on each messageboard and collecting hyperlinks given
there. Links to webpages outside of reddit.com could also be
crawled.&lt;/p&gt;
&lt;p&gt;2. The number of links between webpages could be tabulated in
order to measure the &lt;em&gt;strength&lt;/em&gt; of each link in the network.&lt;/p&gt;
&lt;p&gt;3. Instead of an undirected graph, the direction of the links
could be incorporated into the model.&lt;/p&gt;
&lt;/div&gt;
</content></entry></feed>