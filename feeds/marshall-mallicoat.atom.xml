<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Marshall Mallicoat - Marshall Mallicoat</title><link href="http://marshallmallicoat.com/" rel="alternate"></link><link href="http://marshallmallicoat.com/feeds/marshall-mallicoat.atom.xml" rel="self"></link><id>http://marshallmallicoat.com/</id><updated>2019-02-28T00:00:00-06:00</updated><entry><title>Student Satisfaction Regression</title><link href="http://marshallmallicoat.com/student-satisfaction.html" rel="alternate"></link><published>2019-02-25T00:00:00-06:00</published><updated>2019-02-28T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2019-02-25:/student-satisfaction.html</id><summary type="html">&lt;!-- Style: Student's *t*-test; Mann–Whitney *U* test; Wilcoxon rank-sum test;
(Pearson's) chi-squared test; Smirnov–Kolmogorov test; *p*-value;
*F*-test; en-dash in "Q–Q plot" not hyphen --&gt;
&lt;p&gt;Sears et al. &lt;a class="footnote-reference" href="#sears" id="id1"&gt;[1]&lt;/a&gt; performed a survey of psychology students at the
University of Calgary in order to better understand which factors lead to
students' satisfaction with their degree program. In this project, I perform a
regression analysis on the variables selected for this survey. &lt;a class="footnote-reference" href="#data" id="id2"&gt;[2]&lt;/a&gt; The
code and data …&lt;/p&gt;</summary><content type="html">&lt;!-- Style: Student's *t*-test; Mann–Whitney *U* test; Wilcoxon rank-sum test;
(Pearson's) chi-squared test; Smirnov–Kolmogorov test; *p*-value;
*F*-test; en-dash in "Q–Q plot" not hyphen --&gt;
&lt;p&gt;Sears et al. &lt;a class="footnote-reference" href="#sears" id="id1"&gt;[1]&lt;/a&gt; performed a survey of psychology students at the
University of Calgary in order to better understand which factors lead to
students' satisfaction with their degree program. In this project, I perform a
regression analysis on the variables selected for this survey. &lt;a class="footnote-reference" href="#data" id="id2"&gt;[2]&lt;/a&gt; The
code and data for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/student-satisfaction"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="sears" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Sears, Christopher R., et al. (2017). Predictors of Student
Satisfaction in a Large Psychology Undergraduate Program.  &lt;em&gt;Canadian
Psychology, 58&lt;/em&gt;(2), 148–160. &lt;a class="reference external" href="http://dx.doi.org/10.1037/cap0000082"&gt;http://dx.doi.org/10.1037/cap0000082&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="data" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This analysis uses &lt;em&gt;synthetic&lt;/em&gt; data that was produced to
imitate the data collected in the study itself.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="section" id="exploration"&gt;
&lt;h2&gt;Exploration&lt;/h2&gt;
&lt;p&gt;In the data, we have one response variable (&lt;span class="formula"&gt;&lt;i&gt;y&lt;/i&gt;&lt;/span&gt;) and ten explanatory
variables (&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;, &lt;i&gt;X&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;, …, &lt;i&gt;X&lt;/i&gt;&lt;sub&gt;10&lt;/sub&gt;&lt;/span&gt;). They represent the following
quantities, as assessed by each student in the survey:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;y&lt;/i&gt;&lt;/span&gt;: overall satisfaction with the psychology program&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;: quality of teaching in lectures&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/span&gt;: quality of teaching in labs&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt;: student-faculty interaction&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;4&lt;/sub&gt;&lt;/span&gt;: level of academic challenge&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;: opportunities for research experience&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;6&lt;/sub&gt;&lt;/span&gt;: variety of courses available&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;7&lt;/sub&gt;&lt;/span&gt;: opportunities for class discussions&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;8&lt;/sub&gt;&lt;/span&gt;: opportunities to write about views and ideas&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;9&lt;/sub&gt;&lt;/span&gt;: program advising&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;10&lt;/sub&gt;&lt;/span&gt;: career information&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we read the data into our R program and calculate some descriptive
statistics.&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
&lt;span class="c1"&gt;# Read in data&lt;/span&gt;
df &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'data/survey.csv'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Descriptive statistics&lt;/span&gt;
df_mean &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;sapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;df&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
df_sd &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;sapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;df&lt;span class="p"&gt;,&lt;/span&gt; sd&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;This gives us the mean and standard deviation of each variable:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="6%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;/th&gt;
&lt;th class="head"&gt;Y1&lt;/th&gt;
&lt;th class="head"&gt;X1&lt;/th&gt;
&lt;th class="head"&gt;X2&lt;/th&gt;
&lt;th class="head"&gt;X3&lt;/th&gt;
&lt;th class="head"&gt;X4&lt;/th&gt;
&lt;th class="head"&gt;X5&lt;/th&gt;
&lt;th class="head"&gt;X6&lt;/th&gt;
&lt;th class="head"&gt;X7&lt;/th&gt;
&lt;th class="head"&gt;X8&lt;/th&gt;
&lt;th class="head"&gt;X9&lt;/th&gt;
&lt;th class="head"&gt;X10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;0.076&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;td&gt;-0.046&lt;/td&gt;
&lt;td&gt;0.118&lt;/td&gt;
&lt;td&gt;0.055&lt;/td&gt;
&lt;td&gt;0.148&lt;/td&gt;
&lt;td&gt;-0.051&lt;/td&gt;
&lt;td&gt;-0.097&lt;/td&gt;
&lt;td&gt;0.038&lt;/td&gt;
&lt;td&gt;0.051&lt;/td&gt;
&lt;td&gt;-0.097&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;SD&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;1.6&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;td&gt;1.6&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;1.7&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The answers given to the survey questions were coded as integers from −3 (&lt;em&gt;very
unsatisfied&lt;/em&gt;) to +3 (&lt;em&gt;very satisfied&lt;/em&gt;), with 0 for &lt;em&gt;neutral&lt;/em&gt;. The means of the
variables are all small and near 0, so the students' responses are likely
well-balanced in terms of satisfaction versus dissatisfaction.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="validating-assumptions"&gt;
&lt;h2&gt;Validating Assumptions&lt;/h2&gt;
&lt;p&gt;The linear regression model makes a number of assumptions about the properties
of the data. These include the assumption that the explanatory variables are
linearly independent, or that they lack &lt;strong&gt;perfect&lt;/strong&gt; multicollinearity. To
ensure the soundness of our model, we will check if any of our explanatory are
collinear.&lt;/p&gt;
&lt;p&gt;Besides the theoretical assumption, there are practical reasons to avoid
even &amp;quot;imperfect&amp;quot; collinearity between explanatory variables:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;In a multiple linear regression, we estimate the coefficients with
the formula &lt;span class="formula"&gt;&lt;i&gt;β̂&lt;/i&gt; = (&lt;i&gt;X&lt;/i&gt;&lt;sup&gt;&lt;i&gt;T&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;X&lt;/i&gt;)&lt;sup&gt; − 1&lt;/sup&gt;&lt;i&gt;X&lt;/i&gt;&lt;sup&gt;&lt;i&gt;T&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;y&lt;/i&gt;&lt;/span&gt;. If two
explanatory variables are collinear, the matrix &lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sup&gt;&lt;i&gt;T&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;X&lt;/i&gt;&lt;/span&gt; will
be near singular. This makes it computationally difficult to invert.&lt;/li&gt;
&lt;li&gt;If two explanatory variables are collinear, the variance of
their coefficients in the linear model will be very high.
This reduces the power of the &lt;em&gt;t&lt;/em&gt;-test used to determine
if the coefficient is statistically significant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A crude way to detect collinearity between variables is the look at their
correlation coefficient matrix. Large values (say, greater than 0.70) might
indicate collinearity.&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
df_correl &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; cor&lt;span class="p"&gt;(&lt;/span&gt;df&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The correlation coefficient matrix for our variables is:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="6%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;/th&gt;
&lt;th class="head"&gt;Y1&lt;/th&gt;
&lt;th class="head"&gt;X1&lt;/th&gt;
&lt;th class="head"&gt;X2&lt;/th&gt;
&lt;th class="head"&gt;X3&lt;/th&gt;
&lt;th class="head"&gt;X4&lt;/th&gt;
&lt;th class="head"&gt;X5&lt;/th&gt;
&lt;th class="head"&gt;X6&lt;/th&gt;
&lt;th class="head"&gt;X7&lt;/th&gt;
&lt;th class="head"&gt;X8&lt;/th&gt;
&lt;th class="head"&gt;X9&lt;/th&gt;
&lt;th class="head"&gt;X10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Y1&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;0.40&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X1&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.42&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.42&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.55&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X2&lt;/td&gt;
&lt;td&gt;0.40&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;td&gt;0.18&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X3&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.61&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.47&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X4&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.42&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;0.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X5&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.61&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;0.38&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.37&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X6&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.42&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X7&lt;/td&gt;
&lt;td&gt;0.36&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;0.47&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.38&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X8&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.55&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.46&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X9&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.18&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;0.37&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;0.46&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;X10&lt;/td&gt;
&lt;td&gt;0.45&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;0.35&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;None of the correlation coefficients between the explanatory variables are
larger than 0.70, so there is no obvious sign of collinearity.&lt;/p&gt;
&lt;p&gt;A more thorough way to detect multicollinearity is to compute the Variance Inflation
Factor (VIF) for each variable in the linear model. But to do this, we must first
fit the coefficients of our linear model. This is a one-liner in R:&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
md &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;df&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;span class="formula"&gt;&lt;i&gt;VIF&lt;/i&gt;&lt;sub&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; is defined as &lt;span class="formula"&gt;1 ⁄ (1 − &lt;i&gt;R&lt;/i&gt;&lt;span class="scripts"&gt;&lt;sup class="script"&gt;2&lt;/sup&gt;&lt;sub class="script"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt;)&lt;/span&gt; where &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;span class="scripts"&gt;&lt;sup class="script"&gt;2&lt;/sup&gt;&lt;sub class="script"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;
is the coefficient of determination for explanatory variable &lt;span class="formula"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/span&gt; when
&lt;em&gt;regressed on the other explanatory variables&lt;/em&gt;. We are checking to see
&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; can be predicted to a high degree of accuracy by a linear
combination of the other variables. If &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;span class="scripts"&gt;&lt;sup class="script"&gt;2&lt;/sup&gt;&lt;sub class="script"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt; is large (that is,
that a large proportion of the variance of &lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; can be explained by the
other variables), then &lt;span class="formula"&gt;&lt;i&gt;VIF&lt;/i&gt;&lt;sub&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; will be large. If the VIF for an
explanatory variable is larger than some threshold (say, 5), then we can
conclude that it is collinear and exclude it from our model.&lt;/p&gt;
&lt;p&gt;Another view of the same quantity is &lt;em&gt;tolerance&lt;/em&gt;, which is defined as the
reciprocal of the VIF.  Using the &lt;tt class="docutils literal"&gt;car&lt;/tt&gt; library, we can calculate both of
these for our linear model:&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
vif_md &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; vif&lt;span class="p"&gt;(&lt;/span&gt;md&lt;span class="p"&gt;)&lt;/span&gt;
tol_md &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; vif&lt;span class="p"&gt;(&lt;/span&gt;md&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The VIFs and tolerances for our variables are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="14%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;col width="9%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;/th&gt;
&lt;th class="head"&gt;X1&lt;/th&gt;
&lt;th class="head"&gt;X2&lt;/th&gt;
&lt;th class="head"&gt;X3&lt;/th&gt;
&lt;th class="head"&gt;X4&lt;/th&gt;
&lt;th class="head"&gt;X5&lt;/th&gt;
&lt;th class="head"&gt;X6&lt;/th&gt;
&lt;th class="head"&gt;X7&lt;/th&gt;
&lt;th class="head"&gt;X8&lt;/th&gt;
&lt;th class="head"&gt;X9&lt;/th&gt;
&lt;th class="head"&gt;X10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;VIF&lt;/td&gt;
&lt;td&gt;2.2&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;2.6&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;1.3&lt;/td&gt;
&lt;td&gt;1.6&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;1.7&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tolerance&lt;/td&gt;
&lt;td&gt;0.45&lt;/td&gt;
&lt;td&gt;0.68&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.68&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;0.76&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;0.55&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;None of the VIFs are greater than 5 (or tolerances less than 0.2), so again it
seems that our variables are not collinear.&lt;/p&gt;
&lt;p&gt;Another assumption of the linear regression that we should check is the
normality of the residuals. This can be done using a quantile–quantile (Q–Q)
plot of the empirical distribution of the residuals against a normal
distribution.&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;md&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Q–Q plot of residuals against standard normal distribution" src="./figures/residual-normality.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Our observations fall close to the 45° line, so the residuals
are approximately normal.&lt;/p&gt;
&lt;p&gt;Another assumption to check is the &lt;em&gt;homoscedasticity&lt;/em&gt; of residuals, that is,
that they have the equal variance. We can check this by plotting the each
residual against the value that our model predicts for that observation. If
there is a linear relationship (for example, if the residuals tend to be larger
when the predicted value is larger), then homoscedasticity is violated.&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;md&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of residuals against fitted values" src="./figures/residual-homoscedasticity.png" /&gt;
&lt;/div&gt;
&lt;p&gt;The red fitted line has close to 0 slope, so the residuals
are largely homoscedastic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="selecting-variables-in-model"&gt;
&lt;h2&gt;Selecting Variables in Model&lt;/h2&gt;
&lt;p&gt;Now that we have testing our assumptions, we can assess how well the model fits
the data. To start, we can look at the &lt;em&gt;t&lt;/em&gt;-tests of the coefficients and the
&lt;em&gt;F&lt;/em&gt;-test of the overall model.&lt;/p&gt;
&lt;pre class="code R literal-block"&gt;
&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;md&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre class="code literal-block"&gt;
Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)  0.01406    0.06867   0.205 0.837930
X1           0.32463    0.06375   5.092 7.45e-07 ***
X2           0.02385    0.04661   0.512 0.609407
X3           0.06550    0.05630   1.164 0.245846
X4           0.30920    0.05058   6.113 4.25e-09 ***
X5           0.02068    0.04950   0.418 0.676464
X6          -0.03563    0.03907  -0.912 0.362744
X7          -0.02678    0.05111  -0.524 0.600856
X8           0.17878    0.05331   3.354 0.000935 ***
X9          -0.12363    0.04787  -2.583 0.010434 *
X10          0.11114    0.04760   2.335 0.020424 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.033 on 226 degrees of freedom
Multiple R-squared:  0.568,    Adjusted R-squared:  0.5488
F-statistic: 29.71 on 10 and 226 DF,  p-value: &amp;lt; 2.2e-16
&lt;/pre&gt;
&lt;p&gt;We see that some of our explanatory variables have statistically significant
relationship with the response. The ones that do not we would like to remove,
as long as the fit of our model is not unduly reduced. If the true values for
the coefficients of these variables have small magnitude, then we should be
able to remove them without much harming the fit of our model.&lt;/p&gt;
&lt;p&gt;To accomplish this, we can perform &lt;em&gt;backwards elimination&lt;/em&gt;: one by one, we
remove insignificant variables from our model, testing if it leads to a
statistically significant decrease in the fit of the model. We can test the
significance of the change in &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; between the two models using the
&lt;em&gt;F&lt;/em&gt;-test. We repeat this procedure until all of the variables are significant
(using a significance level of 0.05).&lt;/p&gt;
&lt;p&gt;Applying backwards elimination, we end up with a model that includes the
following variables:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;: quality of teaching in lectures&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;4&lt;/sub&gt;&lt;/span&gt;: level of academic challenge&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;8&lt;/sub&gt;&lt;/span&gt;: opportunities to write about views and ideas&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;9&lt;/sub&gt;&lt;/span&gt;: program advising&lt;/li&gt;
&lt;li&gt;&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;10&lt;/sub&gt;&lt;/span&gt;: career information&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below is the summary of the &lt;em&gt;t&lt;/em&gt;-tests and &lt;em&gt;F&lt;/em&gt;-tests for this final model.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)  0.02603    0.06745   0.386   0.6999
X1           0.34397    0.05484   6.272 1.73e-09 ***
X4           0.31227    0.04710   6.630 2.34e-10 ***
X8           0.19050    0.04560   4.177 4.19e-05 ***
X9          -0.11514    0.04577  -2.516   0.0126 *
X10          0.12527    0.04516   2.774   0.0060 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.029 on 231 degrees of freedom
Multiple R-squared:  0.5617,    Adjusted R-squared:  0.5523
F-statistic: 59.22 on 5 and 231 DF,  p-value: &amp;lt; 2.2e-16
&lt;/pre&gt;
&lt;p&gt;The &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; for the final model is only 0.0063 less than the original. The
adjusted &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;, which removes the spurious increase in &lt;span class="formula"&gt;&lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; that
is caused by adding unpredictive variables, is slightly &lt;em&gt;higher&lt;/em&gt; in the final
model. Backwards elimination gives us a smaller model which explains nearly as
much of the variance in our response variable as the one using all of the
variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;There are some aspects of this regression that have not been addressed:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_data_type"&gt;statistical data type&lt;/a&gt; of our response variable is &lt;em&gt;ordinal&lt;/em&gt;, not
real-valued. The standard linear regression assumes that the response
variable is normally distributed, which is clearly not the case here.
I think, ideally, we would use an &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ordinal_regression"&gt;ordinal regression&lt;/a&gt; instead.&lt;/li&gt;
&lt;li&gt;At the risk of overfitting, we might try capturing interaction between the
explanatory variables by adding cross-product terms to the model like
&lt;span class="formula"&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;&lt;i&gt;i&lt;/i&gt;&lt;/sub&gt;&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; where &lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt; ≠ &lt;i&gt;j&lt;/i&gt;&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content></entry><entry><title>Dental Demographics</title><link href="http://marshallmallicoat.com/dental-demographics.html" rel="alternate"></link><published>2018-12-31T00:00:00-06:00</published><updated>2019-02-11T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-12-31:/dental-demographics.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The profitability of a dental practice depends greatly on its
geographic location. The demographics of the patient population
in the area surrounding the practice affects the demand for
dental care (that is, the need and the means to pay). Key
characteristics are the age and income levels of the …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The profitability of a dental practice depends greatly on its
geographic location. The demographics of the patient population
in the area surrounding the practice affects the demand for
dental care (that is, the need and the means to pay). Key
characteristics are the age and income levels of the population.
Another location-dependent factor is the amount of competition
from other dental practices. An underserved area could be a good
place to start a new practice.&lt;/p&gt;
&lt;p&gt;Most dental practices in the United States are privately owned.
A sole dentist or partnership will finance the opening of a new
dental practice themselves. Placing the practice in a location
that will be profitable is an important part of their business
strategy.&lt;/p&gt;
&lt;p&gt;This is a concern even for dentists who do not own their own
practice. Dentists in the US are usually paid as a percent of the
amount of dental care they provide, which in turn is dependent of
the amount of business the practice can bring in.&lt;/p&gt;
&lt;p&gt;In order to find suitable locations for a new dental practice, I
compiled demographic data from the US Census Bureau and records of
existing practices from online business directories. I confined
my search in the states of Kansas and Missouri, with a particular
focus on the metropolitan area of Kansas City, Missouri.&lt;/p&gt;
&lt;p&gt;All of the code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/dental-demographics"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="american-community-survey"&gt;
&lt;h2&gt;American Community Survey&lt;/h2&gt;
&lt;p&gt;I used demographic data from the &lt;a class="reference external" href="https://www.census.gov/programs-surveys/acs.html"&gt;American Community Survey&lt;/a&gt; (ACS)
published by the US Census Bureau. This survey contains data from
a five-year period, 2012–2016. The attributes collected include
age, sex, race, occupation, veteran status, insurance benefits,
and many others.&lt;/p&gt;
&lt;p&gt;The ACS has some advantages over the &lt;a class="reference external" href="https://www.census.gov/programs-surveys/decennial-census.html"&gt;Decennial Census&lt;/a&gt;. It contains
many more attributes than the Census. The data is also aggregated
and published annually rather than every ten years, providing a
more up-to-date snapshot. One drawback is that the ACS only covers
a sample of the population rather than the population entire.&lt;/p&gt;
&lt;p&gt;In the ACS, the finest granularity of the geographic component
is the Public Use Microdata Area (PUMA). These are generally
small geographic regions, containing no fewer than 100,000 people
(in order to ensure anonymity and statistical credibility). The
boundaries of the PUMAs do not cross state or county lines, so the
data can be aggregated at that level. The boundaries also do not
cross the Census Blocks and Census Tracts used in the Decennial
Census. &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="geographic hierarchy in data" src="./figures/data-geo-hierarchy.jpg" /&gt;
&lt;p class="caption"&gt;The PUMA falls between the County and Census Tract granularities.&lt;/p&gt;
&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;They do, however, cross the boundaries of ZIP codes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="age-and-income"&gt;
&lt;h2&gt;Age and Income&lt;/h2&gt;
&lt;p&gt;Areas with higher incomes will spend more on dental care than
those with lower incomes. This is not surprising, especially in
the US where almost all dental care is paid for out-of-pocket or
by private insurance. &lt;a class="footnote-reference" href="#id6" id="id4"&gt;[2]&lt;/a&gt; Areas with older populations will spend
more on dental care, because older people tend to have more dental
problems and also because of the positive correlation between age
and income/wealth. So, we will focus on these two demographic
attributes in considering a location.&lt;/p&gt;
&lt;p&gt;Below is snippet of the statistics for each PUMA in Kansas (state
code 20 &lt;a class="footnote-reference" href="#id7" id="id5"&gt;[3]&lt;/a&gt;) and Missouri (state code 29).&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="7%" /&gt;
&lt;col width="6%" /&gt;
&lt;col width="13%" /&gt;
&lt;col width="29%" /&gt;
&lt;col width="13%" /&gt;
&lt;col width="30%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;State&lt;/th&gt;
&lt;th class="head"&gt;PUMA&lt;/th&gt;
&lt;th class="head"&gt;Population&lt;/th&gt;
&lt;th class="head"&gt;Median Household Income
(2016 Dollars)&lt;/th&gt;
&lt;th class="head"&gt;Median Age&lt;/th&gt;
&lt;th class="head"&gt;Percent Aged 60 or Older&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;109,867&lt;/td&gt;
&lt;td&gt;$41,200&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;26.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;147,564&lt;/td&gt;
&lt;td&gt;$42,318&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;25.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;134,983&lt;/td&gt;
&lt;td&gt;$32,714&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;12.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;122,120&lt;/td&gt;
&lt;td&gt;$42,564&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;20.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;161,762&lt;/td&gt;
&lt;td&gt;$40,129&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;16.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;116,104&lt;/td&gt;
&lt;td&gt;$81,513&lt;/td&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;td&gt;18.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;602&lt;/td&gt;
&lt;td&gt;153,179&lt;/td&gt;
&lt;td&gt;$65,904&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;22.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;603&lt;/td&gt;
&lt;td&gt;158,524&lt;/td&gt;
&lt;td&gt;$69,349&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;13.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;144,839&lt;/td&gt;
&lt;td&gt;$106,734&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;td&gt;18.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;700&lt;/td&gt;
&lt;td&gt;116,206&lt;/td&gt;
&lt;td&gt;$26,172&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;15.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I also visualized these results using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/QGIS"&gt;QGIS&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Median Household Income" src="./figures/median-household-income.png" /&gt;
&lt;p class="caption"&gt;Higher median household incomes are shown in darker shades.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see that the highest income areas in these two states are
in the suburbs of Kansas City and Saint Louis, on the western
and eastern borders of Missouri. There are also slightly higher
incomes in the suburbs of Topeka (in north–central Kansas) and
Wichita (in south–central Kansas).&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Median Age" src="./figures/median-age.png" /&gt;
&lt;p class="caption"&gt;Higher median ages are shown in darker shades.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The median age is generally higher in more rural regions. In the
areas where large universities are located, we see a much lower
median age than elsewhere. The youthful patches on the map can be
explained by Kansas State University in Manhattan, the University
of Kansas in Lawrence, the University of Missouri in Columbia, and
Missouri State University in Springfield.&lt;/p&gt;
&lt;p&gt;An outlier in income and age is southwest Kansas: it has unusually
high incomes and unusually young population for a rural area.
My guess is that this is due to oil and gas exploitation in the
region, attracting young workers and paying relatively high wages.
See the chart from geological survey below.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="KS Geological Survey" src="./figures/ks_geological_survey_fig3.jpg" /&gt;
&lt;p class="caption"&gt;&lt;a class="reference external" href="http://www.kgs.ku.edu/Publications/PIC/pic32.html"&gt;Kansas Geological Survey, Public Information Circular (PIC) 32, December 2011&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The existing socialized medical programs in the US (e.g.,
Medicaid and Medicare) generally do not provide dental coverage.
One exception is Tricare, which does.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The codes used in the ACS data accord with the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code"&gt;Federal Information Processing Standard&lt;/a&gt;
(FIPS) state codes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="competition"&gt;
&lt;h2&gt;Competition&lt;/h2&gt;
&lt;p&gt;A third consideration is the number of dental practices already
operating the region. The more dental practices there are, the
more competition. This leads to lower utilization of the capacity
of each dental practice and lower profitability.&lt;/p&gt;
&lt;p&gt;To assess this, I scraped the listings of dentists and dental
practices from an online business directory. Since I wanted to
focus on the metropolitan area of Kansas City, I collected listing
from the region around Lenexa, Kansas, an inner-ring suburb. &lt;a class="footnote-reference" href="#id9" id="id8"&gt;[4]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Given the address of each dental practice, I used the &lt;a class="reference external" href="https://www.openstreetmap.org"&gt;Open Street
Map&lt;/a&gt; &lt;a class="reference external" href="https://wiki.openstreetmap.org/wiki/Nominatim"&gt;Nominatim&lt;/a&gt; API to geocode each location, looking up the
latitude and longitude coordinates for each street address.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Shapefile"&gt;shapefiles&lt;/a&gt; provided by the Census Bureau for the PUMAs
contain the boundaries of these geographic regions. Using the
&lt;tt class="docutils literal"&gt;fiona&lt;/tt&gt; Python library, we can easily open and manipulate
shapefiles. In conjunction with the &lt;tt class="docutils literal"&gt;shapely&lt;/tt&gt; library, we can
find which PUMA each of the dental practices is located in, by way
of its coordinates.&lt;/p&gt;
&lt;p&gt;After removing duplicate locations from our list of dental
practices, we can then tabulate the number in practices in each
PUMA. One rule of thumb is that an ideal practice has 2000 active
patients. So, we would be looking for areas with around 2000 or
more people per practice.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Population per Dental Practice" src="./figures/practice-count.png" /&gt;
&lt;p class="caption"&gt;Population per dental practice in the Kansas City area&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;According to our chart, southwest Johnson County (bottom left
PUMA) and western Kansas City (two PUMAs on right) have a suitable
ratio. Wyandotte County (top center PUMA) is somewhat underserved,
so it could be a profitable location for a practice. Southeast
Johnson County (bottom center PUMA) has the lowest ratio; however,
it is also the area with the highest median household income
Kansas or Missouri, so it may nevertheless be a good location.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id9" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id8"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Since I did not scrape the entire business directory, only the
data for the areas close to the origin of Lenexa, KS, can be
expected to be reasonably complete.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="future-directions"&gt;
&lt;h2&gt;Future Directions&lt;/h2&gt;
&lt;p&gt;The attributes considered here may be a good start, but there are
other factors worth looking at. Future analysis could investigate
the dental insurance providers in the area and how much they will
pay for various dental procedures. The same procedure in one area
may be better compensated than in another due to the typical
insurance coverage of the patients.&lt;/p&gt;
&lt;p&gt;The data quality of the existing dental practices could be
improved. The addresses scraped from the business directory were
quite dirty, having problems like:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Some dentists are listed multiple times at different practices&lt;/li&gt;
&lt;li&gt;Some dental practices listed are no longer operating&lt;/li&gt;
&lt;li&gt;Errors in addresses for practices, such as the wrong city or ZIP code given&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also little trust in the listings being complete. In the
future, there may be better sources for this information, such as
lists of in-network dental practitioners published by insurance
companies.&lt;/p&gt;
&lt;p&gt;Open Street Map's Nominatim geocoding API seems to be less
tolerant to malformed addresses than Google Maps'. For a
commercial application, it would probably be worth paying for
Google's service.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Does the Computer Cheat in Mario Party?</title><link href="http://marshallmallicoat.com/mario-party.html" rel="alternate"></link><published>2018-11-17T00:00:00-06:00</published><updated>2018-11-18T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-17:/mario-party.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I happened to play the video game &lt;em&gt;Mario Party 5&lt;/em&gt; with my brother
over a holiday. The game involves rolling dice to move your
character around a playing board. Since we seemed to be getting
lower rolls on average than the computer players, we wondered if
perhaps they were …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I happened to play the video game &lt;em&gt;Mario Party 5&lt;/em&gt; with my brother
over a holiday. The game involves rolling dice to move your
character around a playing board. Since we seemed to be getting
lower rolls on average than the computer players, we wondered if
perhaps they were given an advantage somehow. I manually recorded
a sample of the dice rolls from the game in order to test this
possibility.&lt;/p&gt;
&lt;p&gt;The data and code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/mario-party"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;!-- Style: Student's *t*-test; Mann–Whitney *U* test; Wilcoxon rank-sum test; (Pearson's) chi-squared test; Smirnov–Kolmogorov test; *p*-value --&gt;
&lt;/div&gt;
&lt;div class="section" id="distribution-of-dice-rolls"&gt;
&lt;h2&gt;Distribution of Dice Rolls&lt;/h2&gt;
&lt;p&gt;Each turn, a player rolls a die which returns an integer value
from 1 to 10. Naively, we would assume that the die behaves like
physical dice: that is, that each outcome is equally likely. &lt;a class="footnote-reference" href="#id3" id="id1"&gt;[1]&lt;/a&gt;
The chart below shows the frequencies of the outcomes for the
computer and human players from my sample.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of dice roll frequencies" src="./figures/dice-outcome-frequencies.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Simply looking at the frequencies, it does seem that the computer
players tend to get more very high rolls (9s and 10s) than the
human players and that the human player get more very low rolls
(1s and 2s), but we must do a statistical test to make a rigorous
assessment. We need to perform a goodness of fit test to determine
if our sample likely comes from the assumed distribution: a
discrete uniform distribution with support on [1, 10].&lt;/p&gt;
&lt;p&gt;A simple and effective goodness of fit test is the chi-squared
test. &lt;a class="footnote-reference" href="#id4" id="id2"&gt;[2]&lt;/a&gt; This compares the frequencies for each outcome to the
expected frequency under the null hypothesis that the outcomes are
equally likely. Performing the test, we get the following results:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%" /&gt;
&lt;col width="39%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Player&lt;/th&gt;
&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Human&lt;/td&gt;
&lt;td&gt;6.000&lt;/td&gt;
&lt;td&gt;0.740&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Computer&lt;/td&gt;
&lt;td&gt;5.973&lt;/td&gt;
&lt;td&gt;0.743&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-values are much higher than any reasonable significance
level, so we cannot reject the assumption that the dice is fair
for both the computer and human players.&lt;/p&gt;
&lt;p&gt;Since our sample size is somewhat small, around 40 samples for
each type of player, the expected frequency is around 4 for each
possible outcome. This is less than the rule of thumb that you
should have a minimum expectation of 5 in each category; so,
our test results may not be reliable. We can resolve this by
instead &lt;em&gt;pooling&lt;/em&gt; the results into five categories of outcomes:
rolls of 1 or 2, 3 or 4, 5 or 6, 7 or 8, and 9 or 10. This gives
an expectation for each category of about 8, which should be
sufficient. The new results are then:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%" /&gt;
&lt;col width="39%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Player&lt;/th&gt;
&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Human&lt;/td&gt;
&lt;td&gt;5.500&lt;/td&gt;
&lt;td&gt;0.240&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Computer&lt;/td&gt;
&lt;td&gt;4.757&lt;/td&gt;
&lt;td&gt;0.313&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-values are lower than previously, but the test is still
inconclusive. Both dice appear to be fair.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Additionally, we would expect each roll to be independent of
the others, but I have not tested this hypothesis.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I had first attempted to use the exact test of goodness of
fit. This requires evaluating the CDF of a multinomial
distribution. The CDF of the multinomial distribution
is simply the summation of the pmf evaluated at the
appropriate subset of the sample space. The size of the
sample space is given by a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)#Theorem_two"&gt;theorem in combinatorics&lt;/a&gt;. The number of ways of arranging &lt;em&gt;n&lt;/em&gt; objects into
&lt;em&gt;k&lt;/em&gt; ordered partitions is &lt;em&gt;n + k - 1 choose n&lt;/em&gt;. So, for a
10-dimensional multinomial distribution and a sample size of 40,
this is &lt;em&gt;49 choose 40&lt;/em&gt;, or 2,054,455,634 combinations. To evaluate
the CDF at a particular value, a program would have to iterate
through a loop as many as 2 billion times, calculating the pmf
of one combination each time. Even using the &lt;tt class="docutils literal"&gt;multiprocessing&lt;/tt&gt;
library to take advantage of the two processor cores on my laptop,
I estimate this would take 5.5 hours. I don't think the additional
accuracy of the exact test over the chi-squared test is worth the
computational time.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="mean-dice-roll"&gt;
&lt;h2&gt;Mean Dice Roll&lt;/h2&gt;
&lt;p&gt;Although we are unable to discern that the dice are anything but
fair, the mean dice roll for the computer players is 5.14, which
is slightly higher than the mean for the human players of 4.83.
Even if the outcomes of the dice rolls are close enough to fair
to avoid detection, there might still be some edge give to the
computer player. Instead of testing the distribution, we can
directly test whether the means of the two dice rolls are equal.&lt;/p&gt;
&lt;p&gt;A common test of the means of two population is the Student's
&lt;em&gt;t&lt;/em&gt;-test. This tests whether the difference of the means of the
two populations is significantly different than zero. The null
hypothesis is that the expected difference is exactly zero.&lt;/p&gt;
&lt;p&gt;The results of this test show:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%" /&gt;
&lt;col width="48%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;0.496&lt;/td&gt;
&lt;td&gt;0.621&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this large &lt;em&gt;p&lt;/em&gt;-value, we cannot reject the null hypothesis
that the means are equal.&lt;/p&gt;
&lt;p&gt;The Student's &lt;em&gt;t&lt;/em&gt;-test assumes that the means of the samples are
normally distributed. This will be true asymptotically, proven
in the Central Limit Theorem. &lt;a class="footnote-reference" href="#id6" id="id5"&gt;[3]&lt;/a&gt; Nevertheless, there exists
a nonparametric test which is almost as powerful as Student's
&lt;em&gt;t&lt;/em&gt;-test in many contexts: the Mann–Whitney &lt;em&gt;U&lt;/em&gt; test (also known
as the Wilcoxon sum-rank test). This tests the null hypothesis
that a random sample from one population is equally likely to
be greater than or less than a random sample from a second
population.&lt;/p&gt;
&lt;p&gt;The results of the Mann–Whitney &lt;em&gt;U&lt;/em&gt; test are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%" /&gt;
&lt;col width="48%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Test Statistic&lt;/th&gt;
&lt;th class="head"&gt;&lt;em&gt;p&lt;/em&gt;-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;783.000&lt;/td&gt;
&lt;td&gt;0.663&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Again, these are inconclusive. We cannot reject the hypothesis
that the means are equal.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We have already assumed that the samples are independent,
but for the Central Limit Theorem to hold, the distribution of the
populations must also have finite variances. Since the outcomes of
the dice rolls are confined to the range [1, 10], this will be the
case.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="power-of-statistical-tests"&gt;
&lt;h2&gt;Power of Statistical Tests&lt;/h2&gt;
&lt;!-- Alternate title: Power Analysis --&gt;
&lt;p&gt;It is possible to computer the &lt;em&gt;power&lt;/em&gt; of a statistical test: the
probability of rejecting the null hypothesis given that the null
hypothesis is actually false. The power of a test is a function of
the sample size and the value of the distribution parameters under
the alternate hypothesis. By computing the power of a test, we can
get a sense of if our sample size is sufficiently large to perform
a maximally powerful test.&lt;/p&gt;
&lt;p&gt;Below is a plot of the power curve of Student's &lt;em&gt;t&lt;/em&gt;-test for
various sample sizes.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of power curve for Student's t-test" src="./figures/students-t-power-curve.png" /&gt;
&lt;/div&gt;
&lt;p&gt;We can see that, with a sample size of 77, our test is very
close the power curve of a test with a sample size of 1000.
Increasing the sample size of our test would only marginal
increase the power. Our sample size is large enough for performing
the Student's &lt;em&gt;t&lt;/em&gt;-test at close to its maximum power. &lt;a class="footnote-reference" href="#id8" id="id7"&gt;[4]&lt;/a&gt;&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id8" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Student's &lt;em&gt;t&lt;/em&gt; distribution is asymptotically normal as the
number of the degree of freedom approaches infinity. This is why we
see convergence in the power curves to some upper limit.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- TODO: Plot power curve of chi-squared test as well --&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It seems that the dice in this game are fair and that no advantage
is given to either the computer or human players. Or, if either
of these is not the case, the deviation is small enough that our
tests are not powerful enough to detect it.&lt;/p&gt;
&lt;p&gt;However, there are additional ways that the game may still bend
the rules that we have not tested. For example, if a player
had a run of low rolls, the game might increase the probably
of a higher rolls subsequently, in order to make the game more
forgiving. We could test the assumption that the rolls are indeed
independent of each other. There exist many &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Randomness_tests"&gt;tests of randomness&lt;/a&gt; which can
discern &amp;quot;non-randomness&amp;quot; in a sample, even if the frequencies
of its outcomes match exactly the expected proportions.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Predicting House Prices</title><link href="http://marshallmallicoat.com/kaggle-house-prices.html" rel="alternate"></link><published>2018-11-12T00:00:00-06:00</published><updated>2018-11-12T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-12:/kaggle-house-prices.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The data science company Kaggle administers many predictive
modeling competitions, one of which focuses on &lt;a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"&gt;predicting house prices&lt;/a&gt;.
The problem posed is to predict the price of a house given a large
number of features of the house: the number of stories, the floor
area, the number of bedrooms, the size of the yard, and so on.
The data are from houses in Ames, Iowa, compiled for use in data
science education.&lt;/p&gt;
&lt;p&gt;To solve the problem, I developed a Generalized Linear Regression
(GLM) model. The GLM model works by fitting a function to the
features of houses with known prices; then, to predict the price
of an additional house, the estimated function is evaluated with
the house's particular features. The function takes the form of a
&lt;em&gt;hyperplane&lt;/em&gt; (a generalization of the plane to higher dimensional
space). Usually the response variable, in our case the house
price, is transformed before the fitting takes place.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/kaggle-house-prices"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="data-prep"&gt;
&lt;h2&gt;Data Prep&lt;/h2&gt;
&lt;p&gt;The first step I took was to stratify the data provided,
complete with the price of each house, into training and
cross-validation (CV) datasets. Having a CV dataset lets you
compare the performance of models on out-of-sample data, thereby
avoiding overfitting and getting a more accurate estimate of its
performance. I randomly partitioned the labeled dataset into the
training and CV datasets, since I do not know if there is some
order to how they are presented in the file Kaggle provides which
might bias my model.&lt;/p&gt;
&lt;p&gt;Next, I dealt with any missing values in the dataset. For numeric
variables, I calculated the mean of the variable in the training
data and substituted this value for any missing in the training
and CV datasets. For the categorical variables, I substituted a
new value &amp;quot;Unknown.&amp;quot;&lt;/p&gt;
&lt;p&gt;To select from the many features, I used some simple heuristics.
For the numeric variables, I calculated the variance of each
within the training data. I produced a scatter plot of the
variances of the variables, sorted in ascending order. In this
plot, I found an &amp;quot;elbow&amp;quot; where there was a significant drop-off
in variance. I then selected all variables with variance above
this threshold. This amounted to 12 variables, about a third of
the numeric variables available. The rationale behind this is that
variables with low variance do not provide much discriminating
information between houses, since all of the houses will have
similar values.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of variances of numeric variables" src="./figures/numeric-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the plot of variances of numeric variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For each of the categorical variables, I calculated the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;entropy&lt;/a&gt;,
assuming each value was pulled from a multinomial distribution.
Entropy is a measure of the amount of &amp;quot;information&amp;quot; contained in
a stochastic process. Random variables with little &amp;quot;surprise&amp;quot; in
their realized values will have low entropy. For binary variables,
the entropy calculated is equivalent to the variance of the
corresponding Bernoulli distribution. After calculating the
entropy, I plotted a histogram, found an &amp;quot;elbow&amp;quot; is use for the
threshold, and selected all of the variables above this threshold,
in the same manner as the numeric variables. This amounted to 14
variables, about a third of the categorical variables available.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="plot of entropy of categorical variables" src="./figures/categorical-selection.png" /&gt;
&lt;p class="caption"&gt;&amp;quot;Elbow&amp;quot; in the histogram of entropy of categorical variables&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The selected categorical variables were then encoded as dummy
variables, &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt; so that they can be included in the regression.
One trip-up was that there are values of categorical variables
appearing in the test data that do not appear in the training/CV
data. The universe of values must be known ahead of time in order
to encode the variable as dummy variables. Given a new dataset
containing unseen values of a categorical variable, the model
could not be applied.&lt;/p&gt;
&lt;p&gt;It is somewhat challenging to develop a processing pipeline
that can be applied to all datasets uniformly, particularly
when the processing procedure is &amp;quot;fitted&amp;quot; to the training data.
The processing must then be applied to training data first, the
parameters estimated, and those parameters stored somewhere so
that they can be applied when processing the CV and testing
datasets. Instead of writing out the parameters to disk, as I did
for the scaler and regression parameters, I simply kept them in
memory and processed all of the datasets at once. This is less
than ideal, since I wouldn't have the parameters readily available
to apply to a new dataset, which would be necessary if this model
were used in an application.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Dummy variables are a collection of binary variables whose
combination correspond to one of the values of the categorical
variable. The simplest example is a variable with possible values
&amp;quot;Male&amp;quot; and &amp;quot;Female&amp;quot; being encoded as 1 and 0. For variables with
&lt;em&gt;n&lt;/em&gt; possible values, &lt;em&gt;n - 1&lt;/em&gt; dummy variables are required.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="training"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;Before training the model, I performed some transformations on
the data. I standardized the features, subtracting the mean and
dividing by standard deviation to create features with zero mean
and unit variance. If the features have different scales, the
magnitude of the fitted coefficients in the linear model will be
influenced by the scale of the underlying variables and harder to
compare. Coefficients of variables with a larger scale would also
be penalized more highly if regularization is applied.&lt;/p&gt;
&lt;p&gt;Another transformation was to take the logarithm of the sale
price response variable. There are two reasons for this: First,
like most currency values, the house prices in the data are not
normally distributed, which violates an assumption of the linear
regression. This can be seen in the histogram below over which
I've overlaid a fitted normal distribution.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of house prices" src="./figures/y-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;By log-transforming the response variable, it is much closer to
following a normal distribution. &lt;a class="footnote-reference" href="#id4" id="id3"&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="histogram of log-transformed house prices" src="./figures/y-transformed-hist.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Secondly, the loss function specified for the Kaggle competition
is the mean squared error of the &lt;em&gt;logarithm&lt;/em&gt; of the house prices
predicted. If we wish to develop a model that performs well under
this loss function, we must optimize the parameters of our model
with respect to it.&lt;/p&gt;
&lt;p&gt;My initial model resulted in some very large positive and negative
coefficients in the fitted model. Due to the limited precision
of floating point arithmetic, these coefficients lead to some
overflows and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow"&gt;underflows&lt;/a&gt;, respectively, in the predicted value of
the response variable. To remedy this, I used a ridge regression
instead, which adds a regularization term to the loss function
used for fitting the model, thereby penalizing coefficients with
large magnitude. This solved the problem of unreasonable large
coefficients.&lt;/p&gt;
&lt;p&gt;After training the model, I saved the parameters of both the
standardization procedure and the linear regression. These are
both are needed in order to repeat the preprocessing steps and
make predictions from the CV and test datasets.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Processes that are the sum of many independent occurrences
generally follow a normal distribution, which is consistence with
the Central Limit Theorem. An example of this is human height,
which is perhaps the result of the expression of many different
genes, the quality of nutrition through each phase of childhood,
the effects of childhood disease, etc., which are generally
independence events, each having a small effect. Processes like
prices or salaries cannot be normally distributed on the face
since they cannot have negative values. Secondly, instead of the
constituents having an additive effect, they seem to have more of
a &lt;em&gt;multiplicative&lt;/em&gt; effect on the outcome. Learning two new skills
will increase your salary more than that sum of each alone.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="prediction"&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;p&gt;To make predictions given the CV and test datasets, the
preprocessing steps and repeated:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Standardize the variables using means and standard deviations
from training dataset&lt;/li&gt;
&lt;li&gt;For the CV dataset, log-transform the response variable. (We do
not know the value of the response variable for the testing data,
of course.)&lt;/li&gt;
&lt;li&gt;Apply our regression model to make a prediction: multiply
values of the features by the fitted coefficients, sum these up,
and add the intercept.&lt;/li&gt;
&lt;li&gt;For the CV dataset, calculate the value of the loss function as
a diagnostic.&lt;/li&gt;
&lt;li&gt;Before writing out the predictions, reverse the log-transform
by exponentiating the predicted value.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Kaggle competition is judged by the square root of the mean
squared error (RMSE) of the predictions of the log-transformed
house prices. This metric for our model (on the test dataset) is
0.168, which is fairly middling compared to the leaderboard on the
Kaggle website. For the CV dataset, the metric is 0.166, which is
close to that of the test dataset, as we would expect.&lt;/p&gt;
&lt;p&gt;The metric is somewhat difficult to interpret, so I calculated the
RMSE of the &lt;em&gt;un&lt;/em&gt;-transformed prices for comparison. The RMSE for
the untransformed prices in the CV dataset is $37,576. This is
very roughly &lt;a class="footnote-reference" href="#id6" id="id5"&gt;[3]&lt;/a&gt; the expected deviation of our prediction from
the true price. The mean house price in this dataset is $178,186;
so, although our error is significant, the predictions are within
the ballpark of the true values.&lt;/p&gt;
&lt;p&gt;There are many avenues to explore which could improve the
model's performance. Here are some things to try in the future:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Engineer some custom features, especially ones that capture
interactions between variables. These might be something like the
ratio of bathrooms to bedrooms, or ratio of plot area to house
floor area.&lt;/li&gt;
&lt;li&gt;Make use of the ordinal variables: there are some variables that
are actually ordinal, not categorical. An example of this is X.
Instead of ignoring the ordering of the levels of the variable,
they could be taken advantage of.&lt;/li&gt;
&lt;li&gt;Try some alternate models, especially those that can fit
non-linear functions. There may be some non-linear interactions
between the house price and the independent variables, such as
the price not being monotonically increasing with the value of an
independence variable. One plausible explanation of this might be
something along the lines of: a larger yard may correlate with a
more valuable property, but it may correlate with a more rural
location; the negative effect of the rural location on the house
price might outweigh the increase from the larger yard.&lt;/li&gt;
&lt;li&gt;Supplement external data: we are given the names of
neighborhoods of the houses. There is publicly available data on
houses and their prices from these locations. This data could be
collected and used to supplement the data provided by Kaggle. Or,
a secondary model could be built from the external data and then
combined with the model trained on the Kaggle data in an ensemble.&lt;/li&gt;
&lt;/ul&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The RMSE is in fact the standard deviation of the
residuals, which are the differences between each prediction and
true value. The standard deviation is the square root of the
expected squared deviation, rather than the expected deviation.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content></entry><entry><title>Exploring Subreddit Networks</title><link href="http://marshallmallicoat.com/subreddit-networks.html" rel="alternate"></link><published>2018-11-02T00:00:00-05:00</published><updated>2019-02-19T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-11-02:/subreddit-networks.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The popular website reddit.com contains numerous messageboards
(called &amp;quot;subreddits&amp;quot;), each dedicated to a particular subject,
such as a hobby or topic of interest. Often the users of these
subreddits will place links to related subreddits on the sidebar
of the webpage. In this way, these linked subreddits form a
network of closely related online communities. Because these
lists of links are human-curated, they give high-quality
indicators of related topics and communities. By following these
links programmatically, we can collect the data needed to to
visualize and analyze these networks.&lt;/p&gt;
&lt;p&gt;The code for this project can be found on &lt;a class="reference external" href="https://github.com/mmallicoat/subreddit-graphs"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="scrape-the-data"&gt;
&lt;h2&gt;Scrape the Data&lt;/h2&gt;
&lt;p&gt;I wrote simple web scraper using the Python library &lt;tt class="docutils literal"&gt;scrapy&lt;/tt&gt;
which collects the list of links and other information from each
subreddit page. Starting from an initial subreddit, the scraper
searches the sidebar on the page for any related subreddits that
have been linked there. The scraper then follows those links and
iteratively searches for more links. The name of each subreddit is
collected, along with its description, number of subscribers, and
the list of links to other subreddits.&lt;/p&gt;
&lt;p&gt;From these data, it is possible to construct the network of
subreddits surrounding the initial page. In this network (often
called a &lt;em&gt;graph&lt;/em&gt; in mathematics), each node is a subreddit and
each edge is a hyperlink between them. For simplicity, I decided
to build a non-directed graph, ignoring the directionality of
hyperlinks: a hyperlink from subreddit A to B is treated in the
same manner as a hyperlink from subreddit B to A.&lt;/p&gt;
&lt;p&gt;The scraper uses CSS selectors to locate the desired elements on
each webpage. Here is a code snippet with the selectors:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sub_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subscriber_conversion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-12::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# convert from string to numeric&lt;/span&gt;
    &lt;span class="n"&gt;links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'div.s1s8pi67-0 a::attr(href)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="s1"&gt;'subreddit'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'/r/\w+'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'description'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sidebar&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;css&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p.s34nhbn-14::text'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract_first&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
      &lt;span class="s1"&gt;'links'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="s1"&gt;'subscribers'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sub_count&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The sidebar on the page is identified using the method
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;response.css('div.s1s8pi67-0')&lt;/span&gt;&lt;/tt&gt; where &lt;tt class="docutils literal"&gt;response&lt;/tt&gt; is the
object representing the returned webpage. The string
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;s1s8pi67-0&lt;/span&gt;&lt;/tt&gt; is a unique class of the &lt;tt class="docutils literal"&gt;div&lt;/tt&gt; element containing
the sidebar. &lt;a class="footnote-reference" href="#selectors" id="id1"&gt;[1]&lt;/a&gt; The scaper returns the name of the
subreddit, the subreddit's description, the list of links in the
sidebar, and the number of subscribers.&lt;/p&gt;
&lt;p&gt;I chose two subreddits to use as the starting points to crawl
through related pages, which results in two networks for analysis:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;one centered at /r/programming, which I will refer
to as the &amp;quot;programming network&amp;quot;&lt;/li&gt;
&lt;li&gt;one centered at /r/financialindependence, which I
will refer to as the &amp;quot;financial network.&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these initial subreddits have a fair number of links
listed in their sidebars, which should lead to larger, more
complex networks.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="selectors" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Using these seemingly random attribute values for
the selectors is less than ideal: they are non-semantic and they
seem to change fairly frequently, possibly with each build of the
website. An improvement woudl be to find a more robust selector. I
think it would be possible to use an XPath selector to find the
text &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/r/[subreddit&lt;/span&gt; name]&lt;/tt&gt; that appears in the sidebar, and then
select the document element containing this a few steps up the
hierarchy.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="visualizations"&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;p&gt;After collecting the network data, we can use the library
&lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt; to visualize and analyze the networks. I quickly
made a couple plots using &lt;tt class="docutils literal"&gt;matplotlib&lt;/tt&gt; to visualize the graphs.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="network centered as /r/programming" src="./figures/prog-graph.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Unfortunately, these were difficult to read and not very useful
for exploring the networks. To remedy this, I decided to use the
visualization library &lt;tt class="docutils literal"&gt;d3&lt;/tt&gt; (written in JavaScript) to make some
interactive plots. After we convert the network data into the
&amp;quot;node-link&amp;quot; JSON format using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can read it into a
HTML file containing JavaScript visualizations.&lt;/p&gt;
&lt;p&gt;Click on the images below to view the interactive plots.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/fin-force.html"&gt;&lt;img alt="network centered as /r/financialindependence" src="./figures/fin-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/financialindependence&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;a class="reference external image-reference" href="html/prog-force.html"&gt;&lt;img alt="network centered as /r/programming" src="./figures/prog-force-label.jpg" /&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Plot of the network centered at the subreddit /r/programming&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In these charts, the relative number of subscribers to each
subreddit is represented by the radius of the node (using a
log-scale).&lt;/p&gt;
&lt;p&gt;One of the most salient features is the &amp;quot;spoke-and-hub&amp;quot; structure:
a larger subreddit links to many smaller subreddits, which
are often dedicated to a more specific topic. For example,
/r/financialindependence is linked to country-specific subreddits
for Canada (/r/personalfinancecanada), UK (/r/ukpersonalfinance),
and so on. These can often be sensibly grouped into a cluster of
nodes based on their subject matter. An example of this is the
closely related subreddits surrounding /r/collapse which are all
dedicated to the topic of societal and economic collapse.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis"&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;div class="section" id="average-degree-and-density"&gt;
&lt;h3&gt;Average Degree and Density&lt;/h3&gt;
&lt;p&gt;One descriptive statistic of a graph is the &lt;em&gt;average degree&lt;/em&gt;. The
degree of a node is the number of edges connected to it. The
average degree of a graph is simply the average of the degrees of
each of its nodes. These two networks have low average degrees,
both around 1.3. This is a consequence of the structure of these
networks: there are a small number of &amp;quot;hub&amp;quot; nodes that have links
to a large number of &amp;quot;spoke&amp;quot; nodes, which have few links. As a
result, most of the nodes are &amp;quot;spoke&amp;quot; nodes, usually only having a
single edge. This lack of connections is also shown in another
metric, the &lt;em&gt;graph density&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Graph density is defined as the ratio of the number of edges to
the total possible number of edges between the nodes. The total
possible would be achieved if every node was connected to every
other node. For a graph with &lt;em&gt;n&lt;/em&gt; nodes, this would result in &lt;em&gt;n
choose 2&lt;/em&gt; or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt; edges. The density thus varies from
0 (in a graph with no edges) to 1 (in a graph with every possible
edge). The densities of the financial and programming graphs are
0.01 and 0.04, respectively, so they have low density.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="centrality"&gt;
&lt;h3&gt;Centrality&lt;/h3&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;networkx&lt;/tt&gt;, we can also calculate metrics which helps us
to better understand the network. One property of nodes in a
network that we are interested in is their centrality. The metric
of &lt;em&gt;betweenness centrality&lt;/em&gt; is one way of calculating this.
The betweenness centrality of a node is the proportion of
shortest paths between any other two nodes that pass through
it. &amp;quot;Spoke&amp;quot; nodes will have low values and &amp;quot;hubs&amp;quot; high values.&lt;/p&gt;
&lt;p&gt;The most central nodes in the financial network are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="46%" /&gt;
&lt;col width="41%" /&gt;
&lt;col width="13%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Betweenness Centrality&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/frugal&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapc&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/collapse&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/gamedeals&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/simpleliving&lt;/td&gt;
&lt;td&gt;0.26&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/canadianhardwareswap&lt;/td&gt;
&lt;td&gt;0.24&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/zerowaste&lt;/td&gt;
&lt;td&gt;0.19&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/meditation&lt;/td&gt;
&lt;td&gt;0.17&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/steam&lt;/td&gt;
&lt;td&gt;0.14&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/buildapcsales&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/financialindependence&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;/r/frugal and /r/buildapc are central because they act as a bridge
between the network's two main branches: one focused on financial
matters and the other focused on computer building and gaming.
Because of this, many shortest paths must pass through them.
/r/frugal also unites the main hubs in the financial branch,
/r/collapse, /r/zerowaste, /r/simpleliving, and
/r/financialindependence.&lt;/p&gt;
&lt;p&gt;/r/collapse is a hub for many small subreddits that are not
linked to any other nodes. Any path from one of these nodes to
another other must necessarily pass through /r/collapse,
contributing to its high centrality.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="clustering"&gt;
&lt;h3&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Another metric for describing a network is the &lt;em&gt;clustering
coefficient.&lt;/em&gt; Before we define this, first define a &lt;em&gt;triangle&lt;/em&gt; as
a sub-graph of three nodes that are all connected to each other.
Suppose we have a node &lt;em&gt;u&lt;/em&gt; with degree &lt;em&gt;n&lt;/em&gt;. The maximum possible
of triangles including &lt;em&gt;u&lt;/em&gt; is &lt;em&gt;n choose 2&lt;/em&gt;, or &lt;em&gt;n * (n - 1) / 2&lt;/em&gt;.
The clustering coefficient is the number of existing triangles
including node &lt;em&gt;u&lt;/em&gt; divided by this maximum possible number.
So, this coefficient will always be between 0 and 1. It can be
interpreted as the tendency of a node to cluster with other nodes.
Any node that is only connected to a single other node will always have
a clustering coefficient of 0. If all of a node's neighboring nodes are
connected, then the node will have a clustering coefficient of 1.&lt;/p&gt;
&lt;p&gt;Most of the nodes in our two networks are spokes connected only to
a single hub node and thus will have a clustering coefficient of
0. Nodes with coefficients significantly larger than 0 are more
rare in these networks. This is perhaps not surprising given that
these are sparse graphs.&lt;/p&gt;
&lt;p&gt;The nodes in the programming network with the highest clustering
coefficients are:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="48%" /&gt;
&lt;col width="42%" /&gt;
&lt;col width="10%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Subreddit&lt;/th&gt;
&lt;th class="head"&gt;Clustering Coefficient&lt;/th&gt;
&lt;th class="head"&gt;Edges&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;/r/programmerhumor&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cseducation&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/computerscience&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cryptocurrencymemes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/compsci&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/freelance&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/cs_questions&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/resumes&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/coding&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/javascript&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/experienceddevs&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/learnprogramming&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;/r/jobs&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Many of these have only two or three few neighbors, so the
clustering coefficient of 1 is less significant. In contrast,
while /r/csmajors has a coefficient of only 0.17, it has 12
neighbors: out of the 66 possible triangles, 11 of them are fully
connected. This subreddit be part of something closer to a cluster
than many of nodes with a clustering coefficient of 1.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;There is much room for expansion on this sort of analysis. Some
further avenues to explore are:&lt;/p&gt;
&lt;p&gt;1. A more extensive network could be constructed by crawling the
actual posts on each messageboard and collecting hyperlinks given
there. Links to webpages outside of reddit.com could also be
crawled.&lt;/p&gt;
&lt;p&gt;2. The number of links between webpages could be tabulated in
order to measure the &lt;em&gt;strength&lt;/em&gt; of each link in the network.&lt;/p&gt;
&lt;p&gt;3. Instead of an undirected graph, the direction of the links
could be incorporated into the model.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Lyric Essays</title><link href="http://marshallmallicoat.com/lyric-essays.html" rel="alternate"></link><published>2018-09-22T00:00:00-05:00</published><updated>2018-09-22T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-09-22:/lyric-essays.html</id><summary type="html">&lt;p&gt;Here are a few lyric essays written in 2015:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="../in-the-twilight.html"&gt;In the twilight of GMT+1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../john-henry.html"&gt;John Henry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../junkie.html"&gt;Junkie, don't talk to me&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../thoughts-on-sports.html"&gt;Thoughts on sports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../vanitas.html"&gt;Vanitas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</summary><content type="html">&lt;p&gt;Here are a few lyric essays written in 2015:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="../in-the-twilight.html"&gt;In the twilight of GMT+1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../john-henry.html"&gt;John Henry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../junkie.html"&gt;Junkie, don't talk to me&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../thoughts-on-sports.html"&gt;Thoughts on sports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="../vanitas.html"&gt;Vanitas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content></entry><entry><title>Links to People</title><link href="http://marshallmallicoat.com/links.html" rel="alternate"></link><published>2018-09-12T00:00:00-05:00</published><updated>2018-09-12T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-09-12:/links.html</id><summary type="html">&lt;p&gt;Here is a collection of links to personal websites, either those
I've stumbled upon or those run by people I know.&lt;/p&gt;
&lt;div class="section" id="arts-and-letters"&gt;
&lt;h2&gt;Arts and Letters&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://berliac.com"&gt;Berliac&lt;/a&gt;: comic book artist&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://gabbybess.com/"&gt;Bess, Gabby&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.crispinbest.com/"&gt;Best, Crispin&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.gwern.net/"&gt;Branwen, Gwern&lt;/a&gt;: writer on many topics&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.davidfishkind.com/"&gt;Fishkind, David&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://poemshape.wordpress.com/"&gt;Gillespie, Patrick&lt;/a&gt;: poet&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://erikkennedy.com/"&gt;Kennedy, Erik&lt;/a&gt;: poet&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.audunmortensen.com/"&gt;Mortensen …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Here is a collection of links to personal websites, either those
I've stumbled upon or those run by people I know.&lt;/p&gt;
&lt;div class="section" id="arts-and-letters"&gt;
&lt;h2&gt;Arts and Letters&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://berliac.com"&gt;Berliac&lt;/a&gt;: comic book artist&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://gabbybess.com/"&gt;Bess, Gabby&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.crispinbest.com/"&gt;Best, Crispin&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.gwern.net/"&gt;Branwen, Gwern&lt;/a&gt;: writer on many topics&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.davidfishkind.com/"&gt;Fishkind, David&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://poemshape.wordpress.com/"&gt;Gillespie, Patrick&lt;/a&gt;: poet&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://erikkennedy.com/"&gt;Kennedy, Erik&lt;/a&gt;: poet&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.audunmortensen.com/"&gt;Mortensen, Audun&lt;/a&gt;: artist and poet&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pageantboys.com/"&gt;Pageant Boys&lt;/a&gt;: my friend Alexander Sheppard's band&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://hillarypredko.com/"&gt;Predko, Hillary&lt;/a&gt;: writer and designer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://lkshow.biz/"&gt;Shaw, Lucy K.&lt;/a&gt;: writer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://erikstinson.com/"&gt;Stinson, Erik&lt;/a&gt;: &amp;quot;writer and commercial creative&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.placesiveneverbeen.com/"&gt;Wagenknecht, Addie&lt;/a&gt;: artist and designer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://jtwelsch.com/"&gt;Welsch, J.T.&lt;/a&gt;: &amp;quot;writer and academic&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="computing"&gt;
&lt;h2&gt;Computing&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://zorinaq.com/"&gt;Bevand, Marc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://trevore.com/"&gt;Elkins, Trevor&lt;/a&gt;: programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://blog.alexellis.io/"&gt;Ellis, Alex&lt;/a&gt;: programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://gaxun.net/"&gt;Gaxun&lt;/a&gt;: a raft drifting on the open sea&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://hamberg.no/erlend/"&gt;Hamburg, Erlend&lt;/a&gt;: programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://joeyh.name/"&gt;Hess, Joey&lt;/a&gt;: off-the-grid programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.sacrideo.us/"&gt;Hsu, Aaron&lt;/a&gt;: programmer and
handwriting enthusiast&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://crime.team/~jkap/"&gt;Kaplan, Jae&lt;/a&gt;: founder of shared hosting site &lt;a class="reference external" href="https://crime.team/"&gt;crime.team&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.devever.net/~hl/"&gt;Landau, Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://danluu.com/"&gt;Luu, Dan&lt;/a&gt;: programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://coolguy.website/"&gt;Mandeville, Zach&lt;/a&gt;: solarpunk&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://begriffs.com/"&gt;Nelson, Joe&lt;/a&gt;: programmer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://blog.invisiblethings.org/about/"&gt;Rutkowska, Joanna&lt;/a&gt;: computer engineer&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tilde.town/~vilmibm/"&gt;Smith, Nathaniel&lt;/a&gt;: founder of shared hosting site &lt;a class="reference external" href="https://tilde.town"&gt;tilde.town&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://sheep.horse/"&gt;Stephens, Andrew&lt;/a&gt;: programmer&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="gopher-sites"&gt;
&lt;h2&gt;Gopher Sites &lt;a class="footnote-reference" href="#gopher" id="id1"&gt;[1]&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://sdf.org:70/1/users/dbucklin/"&gt;Bucklin, Dave&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://vernunftzentrum.de/1/index.gph"&gt;C-Keen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://baud.baby"&gt;Cat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://sdf.org/1/users/celadevra"&gt;celadevra&lt;/a&gt;: editor&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://grex.org/1/%7ejandal"&gt;jandal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://sdf.org/1/users/jynx/"&gt;jynx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://circumlunar.space/1/%7esolderpunk/"&gt;solderpunk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="gopher://gopher.black"&gt;Tomasino, James&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table class="docutils footnote" frame="void" id="gopher" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gopher_(protocol)"&gt;Gopher&lt;/a&gt;
is a communication protocol introduced in 1991, the same year as HTTP.
Although far less popular than the Web today,
people still build sites in Gopherspace.
To start reading right away, use the &lt;a class="reference external" href="https://gopher.floodgap.com/gopher/gw"&gt;Floodgap Public Gopher Proxy&lt;/a&gt;.
For a more immersive experience, try the &lt;a class="reference external" href="http://lynx.invisible-island.net/"&gt;Lynx&lt;/a&gt; browser.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content></entry><entry><title>Press and Appearances</title><link href="http://marshallmallicoat.com/press.html" rel="alternate"></link><published>2018-09-05T00:00:00-05:00</published><updated>2018-09-07T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-09-05:/press.html</id><summary type="html">&lt;div class="section" id="press-and-appearances"&gt;
&lt;h2&gt;Press and Appearances&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A reading I gave was covered in Susan Dunne's &lt;a class="reference external" href="http://www.courant.com/entertainment/arts-theater/hc-syllable-poetry-series-little-river-restorative-20171119-story.html"&gt;&amp;quot;Syllable: A Monthly Night Of Words And
Music At Little River Restoratives&amp;quot;&lt;/a&gt; (&lt;em&gt;Hartford Courant&lt;/em&gt;, 28 Nov. 2017).&lt;/li&gt;
&lt;li&gt;I appeared briefly in Adam Humphrey's documentary &lt;a class="reference external" href="https://www.youtube.com/watch?v=Ppm8__FxZ4o"&gt;Shitty Youth&lt;/a&gt; [at the 18:46
minute mark] (Nov. 2012).&lt;/li&gt;
&lt;li&gt;I was profiled in …&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="press-and-appearances"&gt;
&lt;h2&gt;Press and Appearances&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A reading I gave was covered in Susan Dunne's &lt;a class="reference external" href="http://www.courant.com/entertainment/arts-theater/hc-syllable-poetry-series-little-river-restorative-20171119-story.html"&gt;&amp;quot;Syllable: A Monthly Night Of Words And
Music At Little River Restoratives&amp;quot;&lt;/a&gt; (&lt;em&gt;Hartford Courant&lt;/em&gt;, 28 Nov. 2017).&lt;/li&gt;
&lt;li&gt;I appeared briefly in Adam Humphrey's documentary &lt;a class="reference external" href="https://www.youtube.com/watch?v=Ppm8__FxZ4o"&gt;Shitty Youth&lt;/a&gt; [at the 18:46
minute mark] (Nov. 2012).&lt;/li&gt;
&lt;li&gt;I was profiled in Beach Sloth's &lt;a class="reference external" href="http://www.beachsloth.com/marshall-mallicoat-man-myth-legend.html"&gt;&amp;quot;Marshall Mallicoat: Man, Myth, Legend&amp;quot;&lt;/a&gt; (23 Mar. 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content></entry><entry><title>Writing Published Elsewhere</title><link href="http://marshallmallicoat.com/writing-published.html" rel="alternate"></link><published>2018-01-31T00:00:00-06:00</published><updated>2018-11-01T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-01-31:/writing-published.html</id><summary type="html">&lt;ul class="simple"&gt;
&lt;li&gt;&amp;quot;Siste Viator&amp;quot; featured in Susan Dunne's &lt;a class="reference external" href="http://www.courant.com/entertainment/arts-theater/hc-syllable-poetry-series-little-river-restorative-20171119-story.html"&gt;&amp;quot;Syllable: A Monthly Night Of Words
And Music At Little River Restoratives&amp;quot;&lt;/a&gt; (&lt;cite&gt;Hartford Courant&lt;/cite&gt;, 28 Nov. 2017).&lt;/li&gt;
&lt;li&gt;Songs &amp;quot;Cellophane&amp;quot; and &amp;quot;Fountain,&amp;quot; co-written with Alexander Sheppard,
on Pageant Boys' &lt;a class="reference external" href="https://therecordmachine.bandcamp.com/album/shadowboxing"&gt;Shadowboxing&lt;/a&gt; (The Record Machine, 19 May 2017).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://westernbeefs.com/mallicoat"&gt;6 poems&lt;/a&gt; in &lt;cite&gt;Western Beefs of North America&lt;/cite&gt;,
edited …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;ul class="simple"&gt;
&lt;li&gt;&amp;quot;Siste Viator&amp;quot; featured in Susan Dunne's &lt;a class="reference external" href="http://www.courant.com/entertainment/arts-theater/hc-syllable-poetry-series-little-river-restorative-20171119-story.html"&gt;&amp;quot;Syllable: A Monthly Night Of Words
And Music At Little River Restoratives&amp;quot;&lt;/a&gt; (&lt;cite&gt;Hartford Courant&lt;/cite&gt;, 28 Nov. 2017).&lt;/li&gt;
&lt;li&gt;Songs &amp;quot;Cellophane&amp;quot; and &amp;quot;Fountain,&amp;quot; co-written with Alexander Sheppard,
on Pageant Boys' &lt;a class="reference external" href="https://therecordmachine.bandcamp.com/album/shadowboxing"&gt;Shadowboxing&lt;/a&gt; (The Record Machine, 19 May 2017).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://westernbeefs.com/mallicoat"&gt;6 poems&lt;/a&gt; in &lt;cite&gt;Western Beefs of North America&lt;/cite&gt;,
edited by Willis Plummer (Mid 2014).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://newhive.com/popserial/marshall-mallicoat-3-poems"&gt;3 poems&lt;/a&gt; in &lt;cite&gt;Pop Serial&lt;/cite&gt;,
issue 5, edited by Stephen Tully Dierks (Early 2014).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20160530031314/http://www.horseghost.info/p/marshall-mallicoat_8.html"&gt;Essay&lt;/a&gt; in the &amp;quot;detailed accounts of february 15, 2013&amp;quot; issue of &lt;cite&gt;horse ghost&lt;/cite&gt;,
edited by Mathew Donahoo (Late 2013).&lt;/li&gt;
&lt;li&gt;Spoken poem &amp;quot;where my baby at in seattle&amp;quot; in &lt;a class="reference external" href="https://keepthisbagawayfromchildren.bandcamp.com"&gt;Ho Hum Vol. 1&lt;/a&gt;
(&lt;cite&gt;Keep This Bag Away From Children&lt;/cite&gt;, 3 Apr. 2013).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.everyday-genius.com/2013/02/marshall-mallicoat.html"&gt;&amp;quot;Poem&amp;quot;&lt;/a&gt; in
&lt;cite&gt;Everyday Genius&lt;/cite&gt;, edited by Stephen Tully Dierks (15 Feb. 2013).&lt;/li&gt;
&lt;li&gt;Song &amp;quot;Kill Cops, Kill Yourself&amp;quot; in Adam Humphrey's documentary &lt;a class="reference external" href="https://www.youtube.com/watch?v=Ppm8__FxZ4o"&gt;Shitty Youth&lt;/a&gt;
[at the 6:38 minute mark] (Nov. 2012).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20150310015452/http://issue3.popserial.net:80/marshall-mallicoat/"&gt;3 poems&lt;/a&gt;
in &lt;cite&gt;Pop Serial&lt;/cite&gt;, issue 3, edited by Stephen Tully Dierks (Early 2012).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://web.archive.org/web/20160530012856/http://www.horseghost.info/p/marshall-mallicoat.html"&gt;&amp;quot;The radiators come on at night and wake me up&amp;quot;&lt;/a&gt;
in &lt;cite&gt;horse ghost&lt;/cite&gt;, volume one, edited by Matthew Donahoo (Early 2012).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://cooprenner.com/2012/02/Red.html"&gt;4 poems&lt;/a&gt;, co-written with Alexander Sheppard,
in &lt;cite&gt;elimae&lt;/cite&gt;, edited by Coop Renner (Feb. 2012).&lt;/li&gt;
&lt;/ul&gt;
</content></entry><entry><title>Past Readings</title><link href="http://marshallmallicoat.com/readings.html" rel="alternate"></link><published>2018-01-02T00:00:00-06:00</published><updated>2018-09-07T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2018-01-02:/readings.html</id><summary type="html">&lt;div class="contents topic" id="this-is-a-catalog-of-almost-all-of-the-poetry-readings-i-ve-given"&gt;
&lt;p class="topic-title first"&gt;This is a catalog of almost all of the poetry readings I've given.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="#nov-2017-in-hartford-conn" id="id1"&gt;5 Nov. 2017 in Hartford, Conn.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#sept-2013-in-new-york-n-y" id="id2"&gt;27 Sept. 2013 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#jan-2013-in-new-york-n-y" id="id3"&gt;31 Jan. 2013 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#oct-2012-in-new-york-n-y" id="id4"&gt;26 Oct. 2012 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#july-2012-in-new-york-n-y" id="id5"&gt;4 July 2012 in New York, N …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="contents topic" id="this-is-a-catalog-of-almost-all-of-the-poetry-readings-i-ve-given"&gt;
&lt;p class="topic-title first"&gt;This is a catalog of almost all of the poetry readings I've given.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="#nov-2017-in-hartford-conn" id="id1"&gt;5 Nov. 2017 in Hartford, Conn.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#sept-2013-in-new-york-n-y" id="id2"&gt;27 Sept. 2013 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#jan-2013-in-new-york-n-y" id="id3"&gt;31 Jan. 2013 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#oct-2012-in-new-york-n-y" id="id4"&gt;26 Oct. 2012 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#july-2012-in-new-york-n-y" id="id5"&gt;4 July 2012 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#jan-2012-in-new-york-n-y" id="id6"&gt;28 Jan. 2012 in New York, N.Y.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#late-2008-in-lawrence-kan" id="id7"&gt;Late 2008 in Lawrence, Kan.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#apr-2008-in-topeka-kan" id="id8"&gt;27 Apr. 2008 in Topeka, Kan.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#early-2007-in-lawrence-kan" id="id9"&gt;Early 2007 in Lawrence, Kan.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!-- Associated Press Stylebook abbreviations for state and country --&gt;
&lt;div class="section" id="nov-2017-in-hartford-conn"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id1"&gt;5 Nov. 2017 in Hartford, Conn.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Little River Restoratives bar&lt;/li&gt;
&lt;li&gt;Why: &lt;a class="reference external" href="https://syllableseries.wordpress.com/"&gt;Syllable reading series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Other readers: Sonya Huber, Kem Joy Ukwu, songs by Lauren Bolstridge&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Syllable reading series is organized by Julia Pistell and Brett Maddux.
I had emailed Julia some poems and asked if I could participate.
This installment was held at a bar in Frog Hollow, Hartford.&lt;/p&gt;
&lt;p&gt;Sonya Huber teaches as Fairfield University.
She read some essays from a collection focusing on the subject
of chronic pain. She suffers from rheumatoid arthritis.&lt;/p&gt;
&lt;p&gt;I one of here essays, she talks about how her condition makes her skin permeable
to the environment. Pollution, such as smoke/exhaust or chemicals, can actually
cause her pain. What struck me is that, although this sort of damage to the
environment is invisible to most people, she literally &lt;em&gt;feels&lt;/em&gt; it in her body.
There are real victims of pollution, which I didn't appreciate. Also, it
occurred to me that, if we all were similarly sensitive to the environment,
maybe we would work harder to keep it clean. (We surround ourselves with
poisons of our own making, such as xenoestrogens.)&lt;/p&gt;
&lt;p&gt;Kem Joy Ukwu read two short stories. She is from New Jersey.&lt;/p&gt;
&lt;p&gt;Lauren Bolstridge played some songs on acoustic guitar.
She is from West Hartford, CT and lives in the area, I think.&lt;/p&gt;
&lt;p&gt;I talked to Susan Dunne, the Arts Writer from the Hartford Courant.
She wrote an &lt;a class="reference external" href="http://www.courant.com/entertainment/arts-theater/hc-syllable-poetry-series-little-river-restorative-20171119-story.html"&gt;article&lt;/a&gt; about the reading, featuring one of the poems I read.&lt;/p&gt;
&lt;p&gt;At the reading I met some artists and curators who are, perhaps, friends with Brett.
I had a good time talking to them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sept-2013-in-new-york-n-y"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id2"&gt;27 Sept. 2013 in New York, N.Y.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Molasses Books in Brooklyn&lt;/li&gt;
&lt;li&gt;Why: ?&lt;/li&gt;
&lt;li&gt;Other readers: Sarah Jean Alexander, Jonathan Aprea, Connor Messinger,
unknown others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Molasses Books is a single-room bookstore/cafe/bar in Bushwick, Brooklyn.&lt;/p&gt;
&lt;p&gt;I felt like people were reading sort of quietly, so I tried to read real loud.
(I'm not sure if I accomplished this.)
I think I read alright. I was sort of nervous.&lt;/p&gt;
&lt;p&gt;Jonathan Aprea had a line about how at the north pole, there
is no north, and every direction is south. Then he had a line about how
at the center of a condominium, there is no condominium.
I really liked this idea and told him so.&lt;/p&gt;
&lt;p&gt;Afterward we went to a bar (Tutu's) near the &lt;a class="reference external" href="https://mellowpageslibrary.tumblr.com/"&gt;Mellow Pages Library&lt;/a&gt;.
We ate and people drank then everyone left.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="jan-2013-in-new-york-n-y"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id3"&gt;31 Jan. 2013 in New York, N.Y.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Housing Works in Manhattan&lt;/li&gt;
&lt;li&gt;Why: Release of Mira Gonzalez's book
&lt;cite&gt;i will never be beautiful enough to make us beautiful together&lt;/cite&gt;&lt;/li&gt;
&lt;li&gt;Other readers: Mira Gonzalez, Spencer Madsen, Giancarlo DiTrapano,
Kool A.D. (Victor Vazquez), Melissa Broder, Willis Plummer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was the biggest reading I had ever done. Spencer invited me. His press
&lt;em&gt;Sorry House&lt;/em&gt; was publishing Mira's book.&lt;/p&gt;
&lt;p&gt;I wore a polo shirt and a hoodie. The reading went fairly well. I read a thing
about the Gatorade &amp;quot;G-series&amp;quot; which people seemed to think was funny. A video
was recorded by Sam Cooke, but it now seems to be lost. The webpage archived by
&lt;a class="reference external" href="https://web.archive.org/web/20131020095052/http://vimeo.com/user11004662"&gt;archive.org&lt;/a&gt; shows where it was once hosted.&lt;/p&gt;
&lt;p&gt;There was an after-party at a bar.
I was in a weird mood and didn't talk much to people.
I took the train home to Connecticut afterward.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="oct-2012-in-new-york-n-y"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id4"&gt;26 Oct. 2012 in New York, N.Y.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: apartment in Brooklyn&lt;/li&gt;
&lt;li&gt;Why: &lt;cite&gt;Keep This Bag Away From Children&lt;/cite&gt; release&lt;/li&gt;
&lt;li&gt;Other readers: Jordan Castro, David Fishkind, Mallory Whittan,
Andrew Worthington&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This reading was for the release of an issue of
&lt;cite&gt;Keep This Bag Away From Children&lt;/cite&gt;, edited by Andrew Worthington and others.
I was invited to read, although I wasn't published there.
Jordan Castro played a few songs in addition to reading.
I think the reading was possibly at Ed Halladay's apartment.&lt;/p&gt;
&lt;p&gt;Megan Boyle, Tao Lin, and Brandon Scott Gorrell were there.
There was an after-party at Tao's friend's place. It was a pretty nice
apartment. There was a big, open living room where people stood around and talked.
There was maybe a record player set up.&lt;/p&gt;
&lt;p&gt;I took the train home to Connecticut afterward.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="july-2012-in-new-york-n-y"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id5"&gt;4 July 2012 in New York, N.Y.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Mike Bushnell's apartment in Brooklyn&lt;/li&gt;
&lt;li&gt;Why: Fourth of July party&lt;/li&gt;
&lt;li&gt;Other readers: Ana Carrete, Mike Bushnell, Maggie Lee, Caro DeCarlo,
Stephen Michael McDowell (Buttercup McGillicuddy), Mario Ariza, Jackson Nieuwland,
others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This took place at a Fourth of July party on the roof of Mike Bushnell's
apartment in (I believe) Bed—Stuy, Brooklyn.
The reading got started in the evening. It was still hot after the sun went down.&lt;/p&gt;
&lt;p&gt;I remember the reading being pretty rowdy.
There is some coverage of it at &lt;a class="reference external" href="http://internetpeopleinreallife.tumblr.com/post/26469951978/mikes-reading-party-in-brooklyn-with-internet"&gt;this old blog&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="jan-2012-in-new-york-n-y"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id6"&gt;28 Jan. 2012 in New York, N.Y.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Linger café in Brooklyn&lt;/li&gt;
&lt;li&gt;Why: ?&lt;/li&gt;
&lt;li&gt;Other readers: Spencer Madsen, Maggie Lee, Elaine Sun,
Steve Roggenbuck, Poncho Peligroso&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was my first reading in New York City, which was exciting.
I wore a button-up shirt, tucked in, and nice black pants.
I read poems from my (now unavailable) book &lt;cite&gt;Santo Del Supermercado&lt;/cite&gt;,
which was written during a month-long stay at the Hilton Hotel in Stamford, CT.
I was nervous but read alright.&lt;/p&gt;
&lt;p&gt;Poncho read tweets from the recently passed &lt;a class="reference external" href="https://twitter.com/tree_bro"&gt;&amp;#64;tree_bro&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Megan Boyle and Melissa Broder were there.
I arm wrestled with Spencer.
I met his brother Tyler for the first time.&lt;/p&gt;
&lt;p&gt;We went to someone's apartment afterward.
I think I got a little too rowdy.
I remember throwing my shoe across the room.
That night I slept on a rug on the floor of Spencer's tiny room.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="late-2008-in-lawrence-kan"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id7"&gt;Late 2008 in Lawrence, Kan.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: Henry's bar/café&lt;/li&gt;
&lt;li&gt;Why: poetry class&lt;/li&gt;
&lt;li&gt;Other readers: Robert Knapp, Joseph Harrington, unknown others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I took a poetry class taught by Prof. Joseph Harrington
at the University of Kansas. At the end of the semester, all of the students
read together at a local venue. I remember few of their names.&lt;/p&gt;
&lt;p&gt;I read some ecstatic, Ginsberg-inspired stuff.
This is the first time I read with a microphone,
and I didn't really know how to use it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="apr-2008-in-topeka-kan"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id8"&gt;27 Apr. 2008 in Topeka, Kan.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: the RowHouse Restaurant&lt;/li&gt;
&lt;li&gt;Why: the &amp;quot;RowHouse Poetry Revue&amp;quot;&lt;/li&gt;
&lt;li&gt;Other readers: Amy Fleury, Mickey Cesar, Dennis Etzel Jr., Kevin Rabas,
songs by Greg Fox&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Readers from April 28th event: Mike Johnson, Brian Daldorph, Leah Sewell,
d. douglas, Gary Lechliter --&gt;
&lt;p&gt;My recollections are aided by a flyer from the event that my parents had framed.
The reading was sponsored by the Topeka magazine &lt;cite&gt;seveneightfive&lt;/cite&gt;.
It was hosted by Matt Porubsky, an editor of the magazine.
He invited me to read after some of my poems were published there.
He was a former student of Brian Daldorph,
who was a poetry professor at the University of Kansas.
It was Prof. Daldorph who had suggested I submit to the magazine.&lt;/p&gt;
&lt;p&gt;The restaurant itself was located in an actual row house, I think.
I remember the green room being a small kitchen. (I don't know where
the actual cooking was done.)&lt;/p&gt;
&lt;p&gt;This was the first proper reading I did. I was using the pseudonym
Zeke Collyer at the time, inspired by the Collyer brothers.
I was very young, and the other poets were much more accomplished.&lt;/p&gt;
&lt;p&gt;The readers rotated through the rooms of the restaurant, reading
to the diners throughout the meal. My parents were in attendance.
I read poems about masturbation, self-mutilation, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="early-2007-in-lawrence-kan"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id9"&gt;Early 2007 in Lawrence, Kan.&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Where: University of Kansas student union&lt;/li&gt;
&lt;li&gt;Why: open mic&lt;/li&gt;
&lt;li&gt;Other readers: Mickey Cesar, Nick Sprague, Ryan Keast, Devin Lowell, others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was the first reading I did, I think.
It was an open mic event, possibly with some kind of competition element.
It might even have been billed as a &amp;quot;poetry slam.&amp;quot;&lt;/p&gt;
&lt;p&gt;Mickey Caesar won, I think.
He may have got a gift certificate or some other picayune as a prize.&lt;/p&gt;
&lt;p&gt;My friends Nick, Ryan, and Devin also read poems.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>All Along the Livery Line</title><link href="http://marshallmallicoat.com/all-along-the-livery-line.html" rel="alternate"></link><published>2016-11-25T00:00:00-06:00</published><updated>2018-09-07T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2016-11-25:/all-along-the-livery-line.html</id><summary type="html">&lt;p&gt;&lt;cite&gt;All Along the Livery Line&lt;/cite&gt; is a collection of poems published from
2012 to 2014. It is available in PDF &lt;a class="reference external" href="../media/All Along the Livery Line.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;!-- .. _here: {filename}/media/All\ Along\ the\ Livery\ Line.pdf --&gt;
</summary><content type="html">&lt;p&gt;&lt;cite&gt;All Along the Livery Line&lt;/cite&gt; is a collection of poems published from
2012 to 2014. It is available in PDF &lt;a class="reference external" href="../media/All Along the Livery Line.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;!-- .. _here: {filename}/media/All\ Along\ the\ Livery\ Line.pdf --&gt;
</content></entry><entry><title>Vanitas</title><link href="http://marshallmallicoat.com/vanitas.html" rel="alternate"></link><published>2015-07-15T00:00:00-05:00</published><updated>2015-07-15T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2015-07-15:/vanitas.html</id><summary type="html">&lt;div class="section" id="grave-plots"&gt;
&lt;h2&gt;Grave Plots&lt;/h2&gt;
&lt;p&gt;My parents came into a couple grave plots. (I don't know how.) They're
trying to flip them, since they already have spots reserved. They say
that the cemetery in question isn't as desirable as it used to be, so
they want to unload them fast. Especially since cremation …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="grave-plots"&gt;
&lt;h2&gt;Grave Plots&lt;/h2&gt;
&lt;p&gt;My parents came into a couple grave plots. (I don't know how.) They're
trying to flip them, since they already have spots reserved. They say
that the cemetery in question isn't as desirable as it used to be, so
they want to unload them fast. Especially since cremation is getting
more popular.&lt;/p&gt;
&lt;p&gt;My mom on cremation: &lt;em&gt;You don't amount to much.&lt;/em&gt; Just a two-pint Ziploc.
She dumped her mother's ashes illegally off the Pacific coast, by where
the &lt;cite&gt;Twilight&lt;/cite&gt; movies were shot. The remainder was pressed into a bead
that she keeps on her Pandora bracelet. (This one aside, she has about
$1300 in charms on it.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="notice"&gt;
&lt;h2&gt;Notice&lt;/h2&gt;
&lt;p&gt;I've come to understand the things I've left at my parents' house, since
moving out, as &lt;em&gt;horcruxes.&lt;/em&gt; I'll keep them there safe—my old t-shirts
and books—in case I'm cut down in the street. (Tho if I was hit by a
car tomorrow, this and everything else on my computer would be lost
forever.)&lt;/p&gt;
&lt;p&gt;This month I had to delete the first dead person from my email contacts:
an old boss, aged 66, a Marlboro man. Put into the bin and then the bin
emptied.&lt;/p&gt;
&lt;p&gt;I received a Google Alert when my grandfather's Obits4Life page went
live, since I was listed among the bereft. My inheritance remains
obscure. It's like I was left a set of shoe trees, but threw them out,
not knowing what they were for.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="old-man-mallicoat"&gt;
&lt;h2&gt;Old Man Mallicoat&lt;/h2&gt;
&lt;p&gt;I plan to bald gracefully. I'll keep it real short and see if I can
slide by for a couple decades. If it gets too bad, I'll shave it all off
and be done with it.&lt;/p&gt;
&lt;p&gt;My eye doctor says that, if I keep using computers, my eyes will slowly
get worse and worse forever.&lt;/p&gt;
&lt;p&gt;My dentist says that I grind my teeth in my sleep, probably due to
stress. So now I go to sleep in shorts and a mouthguard like a boxer.&lt;/p&gt;
&lt;p&gt;After a weekend in the city, I'll have black snot and my fingers will be
swollen red around the nail. I feel like my body is already rotting.
Like I'm being fitted for the big &amp;amp; tall in the sky.&lt;/p&gt;
&lt;p&gt;For my wake, lay me out on a California king. Run my obituary in the
&lt;cite&gt;Kansas City Star&lt;/cite&gt;, my preferred newspaper of record.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="vanity"&gt;
&lt;h2&gt;Vanity&lt;/h2&gt;
&lt;p&gt;My desk dictionary memorializes Christopher Columbus as thus: &lt;em&gt;1451-1506
Ital. navigator; disc. Am.&lt;/em&gt; I doubt my legacy will be abbreviated so
succinctly.&lt;/p&gt;
&lt;p&gt;They think Los Angeles will been underwater in 100 years. I estimate
I've eaten 4800 peanut butter sandwiches in my lifetime.&lt;/p&gt;
&lt;p&gt;Bell peppers change color from green to yellow or orange and then to
red. I suspect this is the original metaphor for traffic lights.&lt;/p&gt;
&lt;p&gt;Implicit in everything I write here is the assumption that the world has
never ended. Now that it’s been said, I don’t know why that seemed like
such a statement to make.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>John Henry</title><link href="http://marshallmallicoat.com/john-henry.html" rel="alternate"></link><published>2015-06-21T00:00:00-05:00</published><updated>2015-06-21T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2015-06-21:/john-henry.html</id><summary type="html">&lt;div class="section" id="emails-from-your-boss-written-in-the-form-of-a-question-with-a-period-at-the-end"&gt;
&lt;h2&gt;Emails from your boss, written in the form of a question, with a period at the end&lt;/h2&gt;
&lt;p&gt;Smart people are house slaves to the rich. Even among the aliens, the
Nordic whites rule over the little greens and grays. To my landlord, I
am an encumbrance on the property. To …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="emails-from-your-boss-written-in-the-form-of-a-question-with-a-period-at-the-end"&gt;
&lt;h2&gt;Emails from your boss, written in the form of a question, with a period at the end&lt;/h2&gt;
&lt;p&gt;Smart people are house slaves to the rich. Even among the aliens, the
Nordic whites rule over the little greens and grays. To my landlord, I
am an encumbrance on the property. To my employer, a capital outlay. I
find it hard to believe that, in three million years of fossil record,
there is not a single lizard man.&lt;/p&gt;
&lt;p&gt;Some open questions: Why does the cable company need my Social Security
number? Why does my boss need to know my credit score? How come I can't
give &lt;em&gt;him&lt;/em&gt; a piss test? Where do cows get their calcium, if they only
eat grass? And their protein, for that matter?&lt;/p&gt;
&lt;p&gt;Spy satellites are so high up, they always look straight down. If you
face forward, they can only see the top of your head.&lt;/p&gt;
&lt;p&gt;I found a spot behind the security desk at the mall where I can watch
the guards idly browse the internet.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="john-henry"&gt;
&lt;h2&gt;John Henry&lt;/h2&gt;
&lt;p&gt;To survive in white collar labor, you must cultivate a certain
lassitude, and wear it as a suit of armor. If the work load is heavy,
you have to drag your feet, since even if you finish early you won't get
sent home. If the work load is light, you have to pretend to be working
hard or you will get assigned more work. In any case, you must also
appear to be having a good time.&lt;/p&gt;
&lt;p&gt;When the railroads were built, work songs were sung to set the pace of
labor. It kept everyone working at a level of exertion that could be
kept up all day in the hot sun without risking death. It also kept every
working at the same rate, so no one could foolishly work harder to win
the graces of the foreman—which would force everyone else to work
harder, and thereby endanger both their welfare and their lives.&lt;/p&gt;
&lt;p&gt;In the office, there's no sound other than the dull hum of the air
conditioner and the cricketsong of computers. And your coworkers are all
blood doping—cranking on the commute, then drinking themselves to
sleep at night. How can you keep up without joining in?&lt;/p&gt;
&lt;p&gt;John Henry was able to beat the steel-driving machine through sheer effort.
But afterward, his heart gave out and he soon died. The &amp;quot;Ballad of John
Henry&amp;quot; says “That old hammer killed John Henry / but that old hammer
won't kill me”. I'm saying, that old hammer won't kill me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="not-an-apartment-but-a-dark-hole-i-lived-in-for-3-years"&gt;
&lt;h2&gt;Not an apartment, but a dark hole I lived in for 3 years&lt;/h2&gt;
&lt;p&gt;To vacate an apartment without incident feels like getting away with a
crime. You drag your mattress down the stairwell and leave it in a
dumpster like a warm body. Having recouped the full deposit, you skip
town before the landlord gets wise.&lt;/p&gt;
&lt;p&gt;To live is to be in constant fear of legal retribution: speeding,
jaywalking, illegal downloading, cheating on your taxes, and so on. The
state of Connecticut defines “loitering” as: standing around, moving
slowly about, spending time idly, sauntering, dallying, lingering, or
lagging behind. In Florida, twenty grams is a felony.&lt;/p&gt;
&lt;p&gt;Say you found a pair of crystal antlers. How could you explain having a
pair of crystal antlers to the police? Better to play it safe and
quickly dispose of them.&lt;/p&gt;
&lt;p&gt;Whenever you hear sirens, you think it's for you. When you see a cop,
you think &amp;quot;so this is how it ends&amp;quot;. It's only a matter of time before
the black helicopters swoop down to snatch you up. As for me, I know my
last days will be described as &lt;em&gt;brazen&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Junkie, don't talk to me</title><link href="http://marshallmallicoat.com/junkie.html" rel="alternate"></link><published>2015-05-09T00:00:00-05:00</published><updated>2015-05-09T00:00:00-05:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2015-05-09:/junkie.html</id><summary type="html">&lt;div class="section" id="cold-water-after-black-coffee-is-like-tempering-steel"&gt;
&lt;h2&gt;Cold water after black coffee is like tempering steel&lt;/h2&gt;
&lt;p&gt;I've come to a calm in the midst of my mid-morning coffee—my first and
only coffee of the day. The fog in my head burns off. The wool-legged
spiders who cobweb my attic disperse.&lt;/p&gt;
&lt;p&gt;Caffeine doesn't bring euphoria—more like …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="cold-water-after-black-coffee-is-like-tempering-steel"&gt;
&lt;h2&gt;Cold water after black coffee is like tempering steel&lt;/h2&gt;
&lt;p&gt;I've come to a calm in the midst of my mid-morning coffee—my first and
only coffee of the day. The fog in my head burns off. The wool-legged
spiders who cobweb my attic disperse.&lt;/p&gt;
&lt;p&gt;Caffeine doesn't bring euphoria—more like an elasticity to your limbs
and a humming in your head, from the amplified sound of your blood. And
a feeling like being wrapped in warm, clean towels.&lt;/p&gt;
&lt;p&gt;But it's only good for the first few sips. Just as you start to enjoy
the high, you've already peaked. By the time you finish, you're past the
plateau and down in the valley after.&lt;/p&gt;
&lt;p&gt;Nowadays I'm only happy for 15 minutes at a time, followed by a
headache. And with it comes boredom. Not a boredom like watching paint
dry, but watching paint that had recently dried. The grass growing in
the lawn has stalled out, and I'm languishing in a terminal middle-class
lifestyle.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="taking-dexedrine-at-2am-in-nyc-to-hold-you-through-till-5am-when-grand-central-re-opens-and-you-can-get-a-train-back-to-connecticut"&gt;
&lt;h2&gt;Taking Dexedrine at 2am in NYC to hold you through till 5am when Grand Central re-opens and you can get a train back to Connecticut&lt;/h2&gt;
&lt;p&gt;I'd say, if caffeine was a taxi conducting your body through the human
traffic, that Dexedrine is a black sedan speeding through a tunnel at
night.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="junkie"&gt;
&lt;h2&gt;Junkie&lt;/h2&gt;
&lt;p&gt;I can tell when my mom has been drinking coffee. She is jubilant. She is
effervescent. She &lt;em&gt;bubbles&lt;/em&gt;. I tell her, &amp;quot;Don't talk to me, junkie.&amp;quot;&lt;/p&gt;
&lt;p&gt;She started drinking coffee because she was narcoleptic and would fall
asleep while driving. I only started drinking coffee after I started
taking caffeine pills. Which I only took with alcohol, to make drinking
less miserable.&lt;/p&gt;
&lt;p&gt;I read an insightful observation somewhere, maybe in a Gawker comment,
that Lindsay Lohan wasn't a cokehead, just an alcoholic who took cocaine
to be able to drink more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="coffee-as-ideology"&gt;
&lt;h2&gt;Coffee as ideology&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;You're in the city and there's coffee to buy. Either with brunch or
after when you walk around the shops. Later there's beer. Someone's
in town for a few days or it's a going away party. You drink and talk
and make plans to meet at the new brunch place in the morning. They
have the best espresso.&lt;/li&gt;
&lt;li&gt;The pass code for the bathroom at the Union Square Starbucks on July
4, 2014, was 8282. And I gave it away to the first person I saw: a
Russian tourist standing in line. In turn, she gave it to the people
behind her. In this way, she was my first apostle.&lt;/li&gt;
&lt;li&gt;I prefer the discretion of pill poppers and day drunks to the
perverts who go to cafes after dark.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="hairy-man"&gt;
&lt;h2&gt;Hairy Man&lt;/h2&gt;
&lt;p&gt;In the March 2014 issue of &lt;cite&gt;The Red Bulletin&lt;/cite&gt;, an interview with Nicolas
Cage (“Being Nicolas Cage”) opens with “Do you think about death?” (He
does.)&lt;/p&gt;
&lt;p&gt;At the grocery store by the energy drinks, there is a sign that says
&amp;quot;This area is monitored by Closed Circuit Television.&amp;quot; In the aisle by
the safety razors, the same sign, plus a sliding plastic shield across
the shelf—presumably to slow you down when scooping them into your
trash bag.&lt;/p&gt;
&lt;p&gt;The implication is that there exists a cohort of congenitally hairy men
(or one Hairy Man) who both fiends for caffeine and is willing to do
anything to alleviate his condition.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>Thoughts on sports</title><link href="http://marshallmallicoat.com/thoughts-on-sports.html" rel="alternate"></link><published>2015-03-02T00:00:00-06:00</published><updated>2015-03-02T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2015-03-02:/thoughts-on-sports.html</id><summary type="html">&lt;div class="section" id="golf"&gt;
&lt;h2&gt;Golf&lt;/h2&gt;
&lt;p&gt;The inner workings of a miniature golf course amount to a secret gospel.&lt;/p&gt;
&lt;p&gt;Miniature golf is to golf what reality TV is to reality.&lt;/p&gt;
&lt;p&gt;In golf, you swing lowercase L's at the tops of semicolons.&lt;/p&gt;
&lt;p&gt;At the bowling alley, they give you golf pencils to keep score.&lt;/p&gt;
&lt;p&gt;My grandfather …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="golf"&gt;
&lt;h2&gt;Golf&lt;/h2&gt;
&lt;p&gt;The inner workings of a miniature golf course amount to a secret gospel.&lt;/p&gt;
&lt;p&gt;Miniature golf is to golf what reality TV is to reality.&lt;/p&gt;
&lt;p&gt;In golf, you swing lowercase L's at the tops of semicolons.&lt;/p&gt;
&lt;p&gt;At the bowling alley, they give you golf pencils to keep score.&lt;/p&gt;
&lt;p&gt;My grandfather owned both a set of clubs and a bowling ball. My father
has neither.&lt;/p&gt;
&lt;p&gt;I've invented a new drink that's half-Arnold Palmer and half-Shirley
Temple, called the LBJ.&lt;/p&gt;
&lt;p&gt;Guessing a woman's age is a kind of golf.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-sports"&gt;
&lt;h2&gt;Other Sports&lt;/h2&gt;
&lt;p&gt;Baseball is basically unwatchable, but it works on the radio.&lt;/p&gt;
&lt;p&gt;The counter-insurgency is Whac-a-Mole, the drug war Three-card Monte.&lt;/p&gt;
&lt;p&gt;Capitalism has wrought the moneyballing of everything. The Dutch work 6
hours a day, then ride bikes home. Americans work for 6 hours, spend 2
hours on Facebook, then crawl through traffic for 45 minutes.&lt;/p&gt;
&lt;p&gt;Someone who sits very still at a desk all day, looking at small numbers
on a screen, is a kind of athlete. It takes endurance. You need training
and preparation to do it without hurting yourself. These are grown men
with all the hair rubbed off their ankles by socks, and prostates the
size of clenched fists. They will show you their game face.&lt;/p&gt;
&lt;p&gt;The highest compliment you can pay a man is to call him a machine. Men
strive to make themselves into weapons. This is done through emotional
distance, rituals of control, and repetition.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="on-sportsmanship"&gt;
&lt;h2&gt;On Sportsmanship&lt;/h2&gt;
&lt;p&gt;I was a high school wrestler for one day. I was on the swimming team but
quit. After failing to induce vomiting in the bathroom, I lied about
having vomited to get out of practice. At a strip mall dojo, I put my
fist through the drywall.&lt;/p&gt;
&lt;p&gt;In job interviews, I extemporize on my temperament as if I were a race
horse. As long as they let me down easy, I don't care if I don't get a
call back. To be listed among the also-rans is enough for me. I'm like a
retired greyhound who's too skinny to lie down on hardwood floors. I
understand back pain—what I lack is &lt;em&gt;context&lt;/em&gt; for my back pain.&lt;/p&gt;
&lt;p&gt;I saw a picture of the man with the biggest hands in the world, posing
with a basketball. But he doesn't even play basketball. He just holds it
for photographs. It's like that guy who says he's crazy about football,
only he never watches the games, doesn't know the rules, it just pops
into his mind every once in a while.&lt;/p&gt;
&lt;/div&gt;
</content></entry><entry><title>In the twilight of GMT+1</title><link href="http://marshallmallicoat.com/in-the-twilight.html" rel="alternate"></link><published>2015-02-02T00:00:00-06:00</published><updated>2015-02-02T00:00:00-06:00</updated><author><name>Marshall Mallicoat</name></author><id>tag:marshallmallicoat.com,2015-02-02:/in-the-twilight.html</id><summary type="html">&lt;p&gt;The telegraph was an early internet, with its own lolcats and leetspeak.
To people in the future, we probably seem like steampunks: one foot in the
last century, carrying our brains around in our pockets like
Tamagotchis.&lt;/p&gt;
&lt;p&gt;To open a file on your computer, you touch an arrowhead to a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The telegraph was an early internet, with its own lolcats and leetspeak.
To people in the future, we probably seem like steampunks: one foot in the
last century, carrying our brains around in our pockets like
Tamagotchis.&lt;/p&gt;
&lt;p&gt;To open a file on your computer, you touch an arrowhead to a tiny
picture of a manila folder. When you want to save, an artist's rendering
of a 3.5” floppy disk. Clicking on a trashcan will de-index a block of
memory. If the world was Tetris, we'd all have roofs with holes in them
(or no roofs at all).&lt;/p&gt;
&lt;p&gt;Buddhist prayers wheels have a mantra written around so that, when spun,
it is like the mantra is being read. The Dalai Lama has said that having
the mantra on your computer works the same as a traditional wheel. As
the digital image spins on your hard drive, it sends the peaceful prayer
of compassion in all directions.&lt;/p&gt;
&lt;p&gt;The Malaysian space agency recommends that Muslim astronauts, if they
cannot determine the direction of Mecca, pray in the direction of the
Earth. Failing that, any direction is acceptable. Additionally they
advise you to &amp;quot;observe peace with other beings.&amp;quot;&lt;/p&gt;
&lt;p&gt;In Tibet and Nepal, they still practice sky burials, where bodies are
stripped and laid out for the vultures to eat. But the bodies that have
been given medicine or cleaned in hospitals make the vultures sick, so
they have to be buried in the ground.&lt;/p&gt;
&lt;p&gt;Apparently the most coveted donors for knee replacements are Muslim men.
Their patella are kept flexible by the years of praying on their knees.&lt;/p&gt;
&lt;p&gt;There have never been middle-aged rappers before. And you can't even
make a rapper admit to rapping anymore. They've become demure.&lt;/p&gt;
&lt;p&gt;I remember when pay phones cost 35 cents. I remember pay phones. I asked
around and confirmed that they don't even play beer pong with beer in
the cups anymore. They just keep score on paper. We will be among the
last people on Earth to remember before the Internet, if yet we still
do.&lt;/p&gt;
</content></entry></feed>