<!DOCTYPE html>
<html lang="en">
<head>
      <title>Marshall Mallicoat</title>
    <meta charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="http://marshallmallicoat.com/theme/css/style.css" />
 <!-- adds article description to head, if available -->
</head>

<body>
  <header>
      <h1><a href="http://marshallmallicoat.com/" title="Home">Marshall Mallicoat</a></h1>
  </header>

  <main>
<article>
  <header><h1>Student Satisfaction Regression</h1></header>
  <footer>
        Revised
        <time class="modified" datetime="2019-02-28T00:00:00-06:00">
        28 Feb 2019</time>
  </footer>

  <!-- Style: Student's *t*-test; Mann–Whitney *U* test; Wilcoxon rank-sum test;
(Pearson's) chi-squared test; Smirnov–Kolmogorov test; *p*-value;
*F*-test; en-dash in "Q–Q plot" not hyphen -->
<p>Sears et al. <a class="footnote-reference" href="#sears" id="id1">[1]</a> performed a survey of psychology students at the
University of Calgary in order to better understand which factors lead to
students' satisfaction with their degree program. In this project, I perform a
regression analysis on the variables selected for this survey. <a class="footnote-reference" href="#data" id="id2">[2]</a> The
code and data for this project can be found on <a class="reference external" href="https://github.com/mmallicoat/student-satisfaction">GitHub</a>.</p>
<table class="docutils footnote" frame="void" id="sears" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Sears, Christopher R., et al. (2017). Predictors of Student
Satisfaction in a Large Psychology Undergraduate Program.  <em>Canadian
Psychology, 58</em>(2), 148–160. <a class="reference external" href="http://dx.doi.org/10.1037/cap0000082">http://dx.doi.org/10.1037/cap0000082</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="data" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>This analysis uses <em>synthetic</em> data that was produced to
imitate the data collected in the study itself.</td></tr>
</tbody>
</table>
<div class="section" id="exploration">
<h2>Exploration</h2>
<p>In the data, we have one response variable (<span class="formula"><i>y</i></span>) and ten explanatory
variables (<span class="formula"><i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub>10</sub></span>). They represent the following
quantities, as assessed by each student in the survey:</p>
<ul class="simple">
<li><span class="formula"><i>y</i></span>: overall satisfaction with the psychology program</li>
<li><span class="formula"><i>X</i><sub>1</sub></span>: quality of teaching in lectures</li>
<li><span class="formula"><i>X</i><sub>2</sub></span>: quality of teaching in labs</li>
<li><span class="formula"><i>X</i><sub>3</sub></span>: student-faculty interaction</li>
<li><span class="formula"><i>X</i><sub>4</sub></span>: level of academic challenge</li>
<li><span class="formula"><i>X</i><sub>5</sub></span>: opportunities for research experience</li>
<li><span class="formula"><i>X</i><sub>6</sub></span>: variety of courses available</li>
<li><span class="formula"><i>X</i><sub>7</sub></span>: opportunities for class discussions</li>
<li><span class="formula"><i>X</i><sub>8</sub></span>: opportunities to write about views and ideas</li>
<li><span class="formula"><i>X</i><sub>9</sub></span>: program advising</li>
<li><span class="formula"><i>X</i><sub>10</sub></span>: career information</li>
</ul>
<p>First, we read the data into our R program and calculate some descriptive
statistics.</p>
<pre class="code R literal-block">
<span class="c1"># Read in data</span>
df <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">'data/survey.csv'</span><span class="p">)</span>

<span class="c1"># Descriptive statistics</span>
df_mean <span class="o">&lt;-</span> <span class="kp">sapply</span><span class="p">(</span>df<span class="p">,</span> <span class="kp">mean</span><span class="p">)</span>
df_sd <span class="o">&lt;-</span> <span class="kp">sapply</span><span class="p">(</span>df<span class="p">,</span> sd<span class="p">)</span>
</pre>
<p>This gives us the mean and standard deviation of each variable:</p>
<table border="1" class="docutils">
<colgroup>
<col width="6%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">Y1</th>
<th class="head">X1</th>
<th class="head">X2</th>
<th class="head">X3</th>
<th class="head">X4</th>
<th class="head">X5</th>
<th class="head">X6</th>
<th class="head">X7</th>
<th class="head">X8</th>
<th class="head">X9</th>
<th class="head">X10</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Mean</td>
<td>0.076</td>
<td>0.127</td>
<td>-0.046</td>
<td>0.118</td>
<td>0.055</td>
<td>0.148</td>
<td>-0.051</td>
<td>-0.097</td>
<td>0.038</td>
<td>0.051</td>
<td>-0.097</td>
</tr>
<tr><td>SD</td>
<td>1.5</td>
<td>1.6</td>
<td>1.8</td>
<td>1.9</td>
<td>1.6</td>
<td>1.9</td>
<td>2.0</td>
<td>1.7</td>
<td>1.9</td>
<td>1.8</td>
<td>1.9</td>
</tr>
</tbody>
</table>
<p>The answers given to the survey questions were coded as integers from −3 (<em>very
unsatisfied</em>) to +3 (<em>very satisfied</em>), with 0 for <em>neutral</em>. The means of the
variables are all small and near 0, so the students' responses are likely
well-balanced in terms of satisfaction versus dissatisfaction.</p>
</div>
<div class="section" id="validating-assumptions">
<h2>Validating Assumptions</h2>
<p>The linear regression model makes a number of assumptions about the properties
of the data. These include the assumption that the explanatory variables are
linearly independent, or that they lack <strong>perfect</strong> multicollinearity. To
ensure the soundness of our model, we will check if any of our explanatory are
collinear.</p>
<p>Besides the theoretical assumption, there are practical reasons to avoid
even &quot;imperfect&quot; collinearity between explanatory variables:</p>
<ul class="simple">
<li>In a multiple linear regression, we estimate the coefficients with
the formula <span class="formula"><i>β̂</i> = (<i>X</i><sup><i>T</i></sup><i>X</i>)<sup> − 1</sup><i>X</i><sup><i>T</i></sup><i>y</i></span>. If two
explanatory variables are collinear, the matrix <span class="formula"><i>X</i><sup><i>T</i></sup><i>X</i></span> will
be near singular. This makes it computationally difficult to invert.</li>
<li>If two explanatory variables are collinear, the variance of
their coefficients in the linear model will be very high.
This reduces the power of the <em>t</em>-test used to determine
if the coefficient is statistically significant.</li>
</ul>
<p>A crude way to detect collinearity between variables is the look at their
correlation coefficient matrix. Large values (say, greater than 0.70) might
indicate collinearity.</p>
<pre class="code R literal-block">
df_correl <span class="o">&lt;-</span> cor<span class="p">(</span>df<span class="p">)</span>
</pre>
<p>The correlation coefficient matrix for our variables is:</p>
<table border="1" class="docutils">
<colgroup>
<col width="6%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">Y1</th>
<th class="head">X1</th>
<th class="head">X2</th>
<th class="head">X3</th>
<th class="head">X4</th>
<th class="head">X5</th>
<th class="head">X6</th>
<th class="head">X7</th>
<th class="head">X8</th>
<th class="head">X9</th>
<th class="head">X10</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Y1</td>
<td>1.00</td>
<td>0.63</td>
<td>0.40</td>
<td>0.51</td>
<td>0.56</td>
<td>0.44</td>
<td>0.30</td>
<td>0.36</td>
<td>0.54</td>
<td>0.30</td>
<td>0.45</td>
</tr>
<tr><td>X1</td>
<td>0.63</td>
<td>1.00</td>
<td>0.50</td>
<td>0.60</td>
<td>0.42</td>
<td>0.36</td>
<td>0.42</td>
<td>0.39</td>
<td>0.55</td>
<td>0.39</td>
<td>0.41</td>
</tr>
<tr><td>X2</td>
<td>0.40</td>
<td>0.50</td>
<td>1.00</td>
<td>0.34</td>
<td>0.39</td>
<td>0.22</td>
<td>0.31</td>
<td>0.33</td>
<td>0.27</td>
<td>0.18</td>
<td>0.27</td>
</tr>
<tr><td>X3</td>
<td>0.51</td>
<td>0.60</td>
<td>0.34</td>
<td>1.00</td>
<td>0.34</td>
<td>0.61</td>
<td>0.36</td>
<td>0.47</td>
<td>0.60</td>
<td>0.54</td>
<td>0.52</td>
</tr>
<tr><td>X4</td>
<td>0.56</td>
<td>0.42</td>
<td>0.39</td>
<td>0.34</td>
<td>1.00</td>
<td>0.39</td>
<td>0.29</td>
<td>0.34</td>
<td>0.29</td>
<td>0.32</td>
<td>0.35</td>
</tr>
<tr><td>X5</td>
<td>0.44</td>
<td>0.36</td>
<td>0.22</td>
<td>0.61</td>
<td>0.39</td>
<td>1.00</td>
<td>0.28</td>
<td>0.38</td>
<td>0.56</td>
<td>0.37</td>
<td>0.50</td>
</tr>
<tr><td>X6</td>
<td>0.30</td>
<td>0.42</td>
<td>0.31</td>
<td>0.36</td>
<td>0.29</td>
<td>0.28</td>
<td>1.00</td>
<td>0.32</td>
<td>0.34</td>
<td>0.30</td>
<td>0.33</td>
</tr>
<tr><td>X7</td>
<td>0.36</td>
<td>0.39</td>
<td>0.33</td>
<td>0.47</td>
<td>0.34</td>
<td>0.38</td>
<td>0.32</td>
<td>1.00</td>
<td>0.54</td>
<td>0.31</td>
<td>0.25</td>
</tr>
<tr><td>X8</td>
<td>0.54</td>
<td>0.55</td>
<td>0.27</td>
<td>0.60</td>
<td>0.29</td>
<td>0.56</td>
<td>0.34</td>
<td>0.54</td>
<td>1.00</td>
<td>0.46</td>
<td>0.48</td>
</tr>
<tr><td>X9</td>
<td>0.30</td>
<td>0.39</td>
<td>0.18</td>
<td>0.54</td>
<td>0.32</td>
<td>0.37</td>
<td>0.30</td>
<td>0.31</td>
<td>0.46</td>
<td>1.00</td>
<td>0.56</td>
</tr>
<tr><td>X10</td>
<td>0.45</td>
<td>0.41</td>
<td>0.27</td>
<td>0.52</td>
<td>0.35</td>
<td>0.50</td>
<td>0.33</td>
<td>0.25</td>
<td>0.48</td>
<td>0.56</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<p>None of the correlation coefficients between the explanatory variables are
larger than 0.70, so there is no obvious sign of collinearity.</p>
<p>A more thorough way to detect multicollinearity is to compute the Variance Inflation
Factor (VIF) for each variable in the linear model. But to do this, we must first
fit the coefficients of our linear model. This is a one-liner in R:</p>
<pre class="code R literal-block">
md <span class="o">&lt;-</span> lm<span class="p">(</span>df<span class="p">)</span>
</pre>
<p><span class="formula"><i>VIF</i><sub><i>j</i></sub></span> is defined as <span class="formula">1 ⁄ (1 − <i>R</i><span class="scripts"><sup class="script">2</sup><sub class="script"><i>j</i></sub></span>)</span> where <span class="formula"><i>R</i><span class="scripts"><sup class="script">2</sup><sub class="script"><i>j</i></sub></span></span>
is the coefficient of determination for explanatory variable <span class="formula"><i>j</i></span> when
<em>regressed on the other explanatory variables</em>. We are checking to see
<span class="formula"><i>X</i><sub><i>j</i></sub></span> can be predicted to a high degree of accuracy by a linear
combination of the other variables. If <span class="formula"><i>R</i><span class="scripts"><sup class="script">2</sup><sub class="script"><i>j</i></sub></span></span> is large (that is,
that a large proportion of the variance of <span class="formula"><i>X</i><sub><i>j</i></sub></span> can be explained by the
other variables), then <span class="formula"><i>VIF</i><sub><i>j</i></sub></span> will be large. If the VIF for an
explanatory variable is larger than some threshold (say, 5), then we can
conclude that it is collinear and exclude it from our model.</p>
<p>Another view of the same quantity is <em>tolerance</em>, which is defined as the
reciprocal of the VIF.  Using the <tt class="docutils literal">car</tt> library, we can calculate both of
these for our linear model:</p>
<pre class="code R literal-block">
vif_md <span class="o">&lt;-</span> vif<span class="p">(</span>md<span class="p">)</span>
tol_md <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">/</span> vif<span class="p">(</span>md<span class="p">)</span>
</pre>
<p>The VIFs and tolerances for our variables are:</p>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">X1</th>
<th class="head">X2</th>
<th class="head">X3</th>
<th class="head">X4</th>
<th class="head">X5</th>
<th class="head">X6</th>
<th class="head">X7</th>
<th class="head">X8</th>
<th class="head">X9</th>
<th class="head">X10</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>VIF</td>
<td>2.2</td>
<td>1.5</td>
<td>2.6</td>
<td>1.5</td>
<td>2.0</td>
<td>1.3</td>
<td>1.6</td>
<td>2.3</td>
<td>1.7</td>
<td>1.8</td>
</tr>
<tr><td>Tolerance</td>
<td>0.45</td>
<td>0.68</td>
<td>0.39</td>
<td>0.68</td>
<td>0.50</td>
<td>0.76</td>
<td>0.62</td>
<td>0.44</td>
<td>0.58</td>
<td>0.55</td>
</tr>
</tbody>
</table>
<p>None of the VIFs are greater than 5 (or tolerances less than 0.2), so again it
seems that our variables are not collinear.</p>
<p>Another assumption of the linear regression that we should check is the
normality of the residuals. This can be done using a quantile–quantile (Q–Q)
plot of the empirical distribution of the residuals against a normal
distribution.</p>
<pre class="code R literal-block">
plot<span class="p">(</span>md<span class="p">,</span> <span class="m">2</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<img alt="Q–Q plot of residuals against standard normal distribution" src="./figures/residual-normality.png" />
</div>
<p>Our observations fall close to the 45° line, so the residuals
are approximately normal.</p>
<p>Another assumption to check is the <em>homoscedasticity</em> of residuals, that is,
that they have the equal variance. We can check this by plotting the each
residual against the value that our model predicts for that observation. If
there is a linear relationship (for example, if the residuals tend to be larger
when the predicted value is larger), then homoscedasticity is violated.</p>
<pre class="code R literal-block">
plot<span class="p">(</span>md<span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<img alt="plot of residuals against fitted values" src="./figures/residual-homoscedasticity.png" />
</div>
<p>The red fitted line has close to 0 slope, so the residuals
are largely homoscedastic.</p>
</div>
<div class="section" id="selecting-variables-in-model">
<h2>Selecting Variables in Model</h2>
<p>Now that we have testing our assumptions, we can assess how well the model fits
the data. To start, we can look at the <em>t</em>-tests of the coefficients and the
<em>F</em>-test of the overall model.</p>
<pre class="code R literal-block">
<span class="kp">summary</span><span class="p">(</span>md<span class="p">)</span>
</pre>
<pre class="code literal-block">
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.01406    0.06867   0.205 0.837930
X1           0.32463    0.06375   5.092 7.45e-07 ***
X2           0.02385    0.04661   0.512 0.609407
X3           0.06550    0.05630   1.164 0.245846
X4           0.30920    0.05058   6.113 4.25e-09 ***
X5           0.02068    0.04950   0.418 0.676464
X6          -0.03563    0.03907  -0.912 0.362744
X7          -0.02678    0.05111  -0.524 0.600856
X8           0.17878    0.05331   3.354 0.000935 ***
X9          -0.12363    0.04787  -2.583 0.010434 *
X10          0.11114    0.04760   2.335 0.020424 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.033 on 226 degrees of freedom
Multiple R-squared:  0.568,    Adjusted R-squared:  0.5488
F-statistic: 29.71 on 10 and 226 DF,  p-value: &lt; 2.2e-16
</pre>
<p>We see that some of our explanatory variables have statistically significant
relationship with the response. The ones that do not we would like to remove,
as long as the fit of our model is not unduly reduced. If the true values for
the coefficients of these variables have small magnitude, then we should be
able to remove them without much harming the fit of our model.</p>
<p>To accomplish this, we can perform <em>backwards elimination</em>: one by one, we
remove insignificant variables from our model, testing if it leads to a
statistically significant decrease in the fit of the model. We can test the
significance of the change in <span class="formula"><i>R</i><sup>2</sup></span> between the two models using the
<em>F</em>-test. We repeat this procedure until all of the variables are significant
(using a significance level of 0.05).</p>
<p>Applying backwards elimination, we end up with a model that includes the
following variables:</p>
<ul class="simple">
<li><span class="formula"><i>X</i><sub>1</sub></span>: quality of teaching in lectures</li>
<li><span class="formula"><i>X</i><sub>4</sub></span>: level of academic challenge</li>
<li><span class="formula"><i>X</i><sub>8</sub></span>: opportunities to write about views and ideas</li>
<li><span class="formula"><i>X</i><sub>9</sub></span>: program advising</li>
<li><span class="formula"><i>X</i><sub>10</sub></span>: career information</li>
</ul>
<p>Below is the summary of the <em>t</em>-tests and <em>F</em>-tests for this final model.</p>
<pre class="code literal-block">
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.02603    0.06745   0.386   0.6999
X1           0.34397    0.05484   6.272 1.73e-09 ***
X4           0.31227    0.04710   6.630 2.34e-10 ***
X8           0.19050    0.04560   4.177 4.19e-05 ***
X9          -0.11514    0.04577  -2.516   0.0126 *
X10          0.12527    0.04516   2.774   0.0060 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.029 on 231 degrees of freedom
Multiple R-squared:  0.5617,    Adjusted R-squared:  0.5523
F-statistic: 59.22 on 5 and 231 DF,  p-value: &lt; 2.2e-16
</pre>
<p>The <span class="formula"><i>R</i><sup>2</sup></span> for the final model is only 0.0063 less than the original. The
adjusted <span class="formula"><i>R</i><sup>2</sup></span>, which removes the spurious increase in <span class="formula"><i>R</i><sup>2</sup></span> that
is caused by adding unpredictive variables, is slightly <em>higher</em> in the final
model. Backwards elimination gives us a smaller model which explains nearly as
much of the variance in our response variable as the one using all of the
variables.</p>
</div>
<div class="section" id="next-steps">
<h2>Next Steps</h2>
<p>There are some aspects of this regression that have not been addressed:</p>
<ul class="simple">
<li>The <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_data_type">statistical data type</a> of our response variable is <em>ordinal</em>, not
real-valued. The standard linear regression assumes that the response
variable is normally distributed, which is clearly not the case here.
I think, ideally, we would use an <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinal_regression">ordinal regression</a> instead.</li>
<li>At the risk of overfitting, we might try capturing interaction between the
explanatory variables by adding cross-product terms to the model like
<span class="formula"><i>X</i><sub><i>i</i></sub><i>X</i><sub><i>j</i></sub></span> where <span class="formula"><i>i</i> ≠ <i>j</i></span>.</li>
</ul>
</div>


</article>
  </main>

  <hr>
  <footer>
  <nav>
    <h3>Navigation</h3>
    <ul>
      <li><a href="http://marshallmallicoat.com/">Home</a></li>
      <li><a href="http://marshallmallicoat.com/about.html">About</a></li>
      <li><a href="http://marshallmallicoat.com/feed.xml">Feed</a></li>
    </ul>
  </nav>
  </footer>
</body>
</html>